# Projet Data Science : Credit Scoring Bancaire

## Informations du Projet

<div align="center">
  <img src="https://drive.google.com/uc?export=view&id=1SQpPypsE5ooAj5cJpnNIDXx3qIRhF8Rq" alt="Photo de l'auteur" width="200" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
</div>

**Auteur** : Wiam El khoudri  
**Email** : wiamelkhoudri@gmail.com  
**Module** : Data Science & Machine Learning  
**Ann√©e Universitaire** : 2025-2026  
**Enseignant** : A. Larhlimi  
**Th√©matique** : Finance - Credit Scoring  
**Date de soumission** : Janvier 2026

---

## üìã Sommaire

1. [Introduction](#1-introduction)
   - 1.1 [Contexte de la Mission](#11-contexte-de-la-mission)
   - 1.2 [Probl√©matique](#12-probl√©matique)
   - 1.3 [Objectifs du Projet](#13-objectifs-du-projet)
2. [Th√©matique : Credit Scoring](#2-th√©matique--credit-scoring)
   - 2.1 [D√©finition du Credit Scoring](#21-d√©finition-du-credit-scoring)
   - 2.2 [Enjeux Business](#22-enjeux-business)
   - 2.3 [Type de Machine Learning](#23-type-de-machine-learning)
3. [Pr√©sentation du Dataset](#3-pr√©sentation-du-dataset)
   - 3.1 [Source des Donn√©es](#31-source-des-donn√©es)
   - 3.2 [Description G√©n√©rale](#32-description-g√©n√©rale)
   - 3.3 [Dictionnaire des Variables](#33-dictionnaire-des-variables)
   - 3.4 [Variable Cible](#34-variable-cible)
4. [M√©thodologie](#4-m√©thodologie)
   - 4.1 [Pipeline de Travail](#41-pipeline-de-travail)
   - 4.2 [Outils et Technologies](#42-outils-et-technologies)
5. [Pr√©traitement des Donn√©es](#5-pr√©traitement-des-donn√©es)
   - 5.1 [Nettoyage des Donn√©es](#51-nettoyage-des-donn√©es)
   - 5.2 [Gestion des Valeurs Manquantes](#52-gestion-des-valeurs-manquantes)
   - 5.3 [Encodage des Variables Cat√©gorielles](#53-encodage-des-variables-cat√©gorielles)
   - 5.4 [Normalisation et Standardisation](#54-normalisation-et-standardisation)
6. [Analyse Exploratoire des Donn√©es (EDA)](#6-analyse-exploratoire-des-donn√©es-eda)
   - 6.1 [Statistiques Descriptives](#61-statistiques-descriptives)
   - 6.2 [Visualisation des Distributions](#62-visualisation-des-distributions)
   - 6.3 [Analyse des Corr√©lations](#63-analyse-des-corr√©lations)
   - 6.4 [Feature Engineering](#64-feature-engineering)
7. [Mod√©lisation Machine Learning](#7-mod√©lisation-machine-learning)
   - 7.1 [S√©paration Train/Test](#71-s√©paration-traintest)
   - 7.2 [S√©lection des Algorithmes](#72-s√©lection-des-algorithmes)
   - 7.3 [Validation Crois√©e](#73-validation-crois√©e)
   - 7.4 [Optimisation des Hyperparam√®tres](#74-optimisation-des-hyperparam√®tres)
8. [R√©sultats et Discussion](#8-r√©sultats-et-discussion)
   - 8.1 [M√©triques de Performance](#81-m√©triques-de-performance)
   - 8.2 [Comparaison des Mod√®les](#82-comparaison-des-mod√®les)
   - 8.3 [Analyse des Erreurs](#83-analyse-des-erreurs)
   - 8.4 [Interpr√©tabilit√© du Mod√®le](#84-interpr√©tabilit√©-du-mod√®le)
9. [Conclusion](#9-conclusion)
   - 9.1 [Synth√®se des R√©sultats](#91-synth√®se-des-r√©sultats)
   - 9.2 [Limites du Mod√®le](#92-limites-du-mod√®le)
   - 9.3 [Pistes d'Am√©lioration](#93-pistes-dam√©lioration)
10. [R√©f√©rences](#10-r√©f√©rences)
11. [Annexes](#11-annexes)

---

## 1. Introduction

### 1.1 Contexte de la Mission

Dans le cadre du module Data Science & Machine Learning de l'ann√©e universitaire 2025-2026, ce projet nous place dans la position d'un Data Scientist au sein d'un cabinet d'√©tudes strat√©giques sp√©cialis√© dans le secteur financier. La mission consiste √† d√©velopper un syst√®me de credit scoring permettant d'√©valuer automatiquement la solvabilit√© des clients demandeurs de cr√©dit.

Le secteur bancaire fait face √† un d√©fi majeur : accorder des cr√©dits tout en minimisant le risque de d√©faut de paiement. Chaque ann√©e, les pertes li√©es aux cr√©dits non rembours√©s repr√©sentent des milliards d'euros pour les institutions financi√®res. Dans ce contexte, les techniques de Machine Learning offrent des opportunit√©s consid√©rables pour am√©liorer la pr√©cision des d√©cisions d'octroi de cr√©dit.

### 1.2 Probl√©matique

**Question centrale** : Comment pr√©dire avec pr√©cision si un client sera en d√©faut de paiement ou non, sur la base de ses caract√©ristiques d√©mographiques, financi√®res et comportementales ?

Cette probl√©matique soul√®ve plusieurs sous-questions :
- Quelles sont les variables les plus pr√©dictives du risque de d√©faut ?
- Comment traiter le d√©s√©quilibre potentiel entre bons et mauvais payeurs ?
- Quel mod√®le offre le meilleur compromis entre performance et interpr√©tabilit√© ?
- Comment minimiser les erreurs co√ªteuses (faux n√©gatifs) tout en √©vitant de rejeter des clients solvables (faux positifs) ?

### 1.3 Objectifs du Projet

**Objectif principal** : D√©velopper un mod√®le de classification binaire capable de pr√©dire le risque de d√©faut de paiement avec une pr√©cision sup√©rieure √† 80% (AUC-ROC).

**Objectifs secondaires** :
1. **Exploration** : Comprendre les patterns et relations dans les donn√©es de cr√©dit
2. **Transformation** : Nettoyer et pr√©parer les donn√©es pour la mod√©lisation
3. **Mod√©lisation** : Comparer au moins trois algorithmes de Machine Learning diff√©rents
4. **Optimisation** : Affiner les hyperparam√®tres pour maximiser les performances
5. **Interpr√©tation** : Identifier les facteurs cl√©s influen√ßant le risque de cr√©dit
6. **Communication** : Pr√©senter les r√©sultats de mani√®re claire et exploitable

---

## 2. Th√©matique : Credit Scoring

### 2.1 D√©finition du Credit Scoring

Le credit scoring est une m√©thode statistique permettant d'√©valuer la probabilit√© qu'un emprunteur rembourse son cr√©dit. Il s'agit d'attribuer un score num√©rique √† chaque demandeur, refl√©tant son niveau de risque. Plus le score est √©lev√©, plus le client est consid√©r√© comme fiable.

Cette technique est utilis√©e pour :
- L'octroi de pr√™ts personnels et immobiliers
- L'attribution de cartes de cr√©dit
- Le calcul des taux d'int√©r√™t personnalis√©s
- La gestion du portefeuille de cr√©dit

### 2.2 Enjeux Business

#### Pour la Banque
- **R√©duction des pertes** : Diminution du taux de d√©faut de paiement
- **Optimisation du capital** : Allocation efficace des ressources financi√®res
- **Automatisation** : Acc√©l√©ration du processus de d√©cision (de plusieurs jours √† quelques minutes)
- **Conformit√© r√©glementaire** : Respect des normes B√¢le II/III

#### Pour les Clients
- **Rapidit√©** : R√©ponse instantan√©e sur l'√©ligibilit√© au cr√©dit
- **√âquit√©** : D√©cisions bas√©es sur des crit√®res objectifs et mesurables
- **Personnalisation** : Offres adapt√©es au profil de risque

#### Impact √âconomique
Le march√© mondial du cr√©dit repr√©sente plusieurs trillions d'euros. Une am√©lioration de 1% dans la pr√©diction du risque peut g√©n√©rer des √©conomies de plusieurs millions d'euros pour une institution financi√®re de taille moyenne.

### 2.3 Type de Machine Learning

**Classification Binaire Supervis√©e**

- **Type** : Apprentissage supervis√© (Supervised Learning)
- **Cat√©gorie** : Classification binaire
- **Classes** : 
  - Classe 0 : Bon payeur (pas de d√©faut)
  - Classe 1 : Mauvais payeur (d√©faut de paiement)
- **Input** : Features (variables explicatives) sur le profil client
- **Output** : Probabilit√© de d√©faut et classe pr√©dite (0 ou 1)

**Justification du choix** : La nature binaire du probl√®me (d√©faut/pas de d√©faut) et la disponibilit√© de donn√©es historiques √©tiquet√©es en font un cas typique de classification supervis√©e.

---

## 3. Pr√©sentation du Dataset

### 3.1 Source des Donn√©es

**Origine** : Kaggle - "Credit Scoring for Borrowers in Bank"  
**Auteur** : kapturovalexander  
**URL** : https://www.kaggle.com/datasets/kapturovalexander/bank-credit-scoring/data  
**Licence** : [√Ä pr√©ciser selon Kaggle]  
**Date de collecte** : [√Ä pr√©ciser]

**Justification du choix** : Ce dataset a √©t√© s√©lectionn√© pour plusieurs raisons :
- Richesse des variables (d√©mographiques, financi√®res, comportementales)
- Taille suffisante pour l'entra√Ænement de mod√®les robustes
- Probl√©matique r√©aliste et applicable en contexte professionnel
- Complexit√© adapt√©e au niveau du module

### 3.2 Description G√©n√©rale

**M√©tadonn√©es du Dataset** :

| Caract√©ristique | Valeur |
|-----------------|--------|
| Nombre d'observations | [√Ä compl√©ter apr√®s chargement] |
| Nombre de variables | [√Ä compl√©ter apr√®s chargement] |
| Variables num√©riques | [√Ä compl√©ter] |
| Variables cat√©gorielles | [√Ä compl√©ter] |
| Variable cible | [Nom de la variable] |
| P√©riode couverte | [Si applicable] |
| Taux de valeurs manquantes | [√Ä calculer] |
| D√©s√©quilibre des classes | [Ratio bon/mauvais payeurs] |

### 3.3 Dictionnaire des Variables

Le dataset contient g√©n√©ralement les types de variables suivants (√† adapter selon le dataset r√©el) :

#### Variables D√©mographiques

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `age` | Num√©rique | √Çge du client en ann√©es | 25, 45, 62 |
| `gender` | Cat√©gorielle | Genre du client | M, F |
| `marital_status` | Cat√©gorielle | Statut marital | Single, Married, Divorced |
| `dependents` | Num√©rique | Nombre de personnes √† charge | 0, 1, 2, 3+ |
| `education` | Cat√©gorielle | Niveau d'√©ducation | High School, Bachelor, Master, PhD |

#### Variables Professionnelles

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `employment_type` | Cat√©gorielle | Type d'emploi | Full-time, Part-time, Self-employed |
| `job_tenure` | Num√©rique | Anciennet√© dans l'emploi (ann√©es) | 1, 5, 10 |
| `income` | Num√©rique | Revenu mensuel/annuel (‚Ç¨) | 2000, 3500, 5000 |
| `industry` | Cat√©gorielle | Secteur d'activit√© | IT, Finance, Healthcare |

#### Variables Financi√®res

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `loan_amount` | Num√©rique | Montant du pr√™t demand√© (‚Ç¨) | 5000, 15000, 30000 |
| `loan_term` | Num√©rique | Dur√©e du pr√™t (mois) | 12, 24, 36, 60 |
| `interest_rate` | Num√©rique | Taux d'int√©r√™t (%) | 3.5, 5.2, 7.8 |
| `debt_to_income` | Num√©rique | Ratio dette/revenu | 0.2, 0.35, 0.5 |
| `credit_history_length` | Num√©rique | Anciennet√© historique cr√©dit (ann√©es) | 3, 7, 15 |

#### Variables Comportementales

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `num_credit_lines` | Num√©rique | Nombre de lignes de cr√©dit | 1, 3, 5 |
| `num_late_payments` | Num√©rique | Nombre de retards de paiement | 0, 1, 2 |
| `has_mortgage` | Binaire | Poss√®de un pr√™t immobilier | 0 (Non), 1 (Oui) |
| `has_car_loan` | Binaire | Poss√®de un pr√™t auto | 0 (Non), 1 (Oui) |
| `credit_utilization` | Num√©rique | Taux d'utilisation du cr√©dit (%) | 15, 45, 80 |

### 3.4 Variable Cible

**Nom** : `default` (ou √©quivalent selon le dataset)  
**Type** : Binaire (0/1)  
**Signification** :
- **0** : Client sans d√©faut de paiement (bon payeur)
- **1** : Client en d√©faut de paiement (mauvais payeur)

**D√©finition du d√©faut** : Un d√©faut de paiement est g√©n√©ralement d√©fini comme un retard de paiement sup√©rieur √† 90 jours cons√©cutifs.

---

## 4. M√©thodologie

### 4.1 Pipeline de Travail

Notre approche suit le cycle de vie standard d'un projet de Machine Learning :

```
1. Compr√©hension du probl√®me business
   ‚Üì
2. Collecte et exploration des donn√©es (EDA)
   ‚Üì
3. Nettoyage et pr√©traitement
   ‚Üì
4. Feature Engineering
   ‚Üì
5. S√©paration des donn√©es (Train/Validation/Test)
   ‚Üì
6. Entra√Ænement de mod√®les multiples
   ‚Üì
7. Validation crois√©e
   ‚Üì
8. Optimisation des hyperparam√®tres
   ‚Üì
9. √âvaluation et comparaison des mod√®les
   ‚Üì
10. S√©lection du meilleur mod√®le
   ‚Üì
11. √âvaluation finale sur le jeu de test
   ‚Üì
12. Interpr√©tation et analyse
   ‚Üì
13. Documentation et pr√©sentation
```

### 4.2 Outils et Technologies

**Langage** : Python 3.10+

**Biblioth√®ques principales** :

```python
# Manipulation de donn√©es
pandas==2.1.0
numpy==1.24.0

# Visualisation
matplotlib==3.7.1
seaborn==0.12.2
plotly==5.14.1

# Preprocessing
scikit-learn==1.3.0
imbalanced-learn==0.11.0

# Mod√©lisation
xgboost==1.7.6
lightgbm==4.0.0
catboost==1.2

# √âvaluation et interpr√©tabilit√©
shap==0.42.1
lime==0.2.0.1

# Utilitaires
jupyter==1.0.0
tqdm==4.65.0
```

**Environnement de d√©veloppement** :
- Jupyter Notebook pour l'exploration interactive
- Git/GitHub pour le versioning
- Visual Studio Code pour l'√©dition de code

---

## 5. Pr√©traitement des Donn√©es

### 5.1 Nettoyage des Donn√©es

**Objectif** : Garantir la qualit√© et la coh√©rence des donn√©es avant toute analyse.

#### 5.1.1 D√©tection et Suppression des Doublons

**Strat√©gie** :
```python
# Identification des doublons
duplicates = df.duplicated().sum()
print(f"Nombre de lignes dupliqu√©es : {duplicates}")

# Suppression des doublons
df_clean = df.drop_duplicates()
```

**Justification** : Les doublons peuvent biaiser les statistiques et la performance du mod√®le. Ils sont supprim√©s sauf si justifi√©s business (ex : un client ayant plusieurs pr√™ts).

#### 5.1.2 Formatage des Donn√©es

**Actions r√©alis√©es** :
- Conversion des types de donn√©es (ex : strings ‚Üí numeric)
- Standardisation des formats de dates
- Correction des valeurs aberrantes √©videntes (ex : √¢ge n√©gatif)
- Uniformisation des cat√©gories (ex : "Male"/"M" ‚Üí "M")

#### 5.1.3 D√©tection des Outliers

**M√©thode IQR (Interquartile Range)** :
```python
Q1 = df['income'].quantile(0.25)
Q3 = df['income'].quantile(0.75)
IQR = Q3 - Q1
outliers = (df['income'] < Q1 - 1.5*IQR) | (df['income'] > Q3 + 1.5*IQR)
```

**Traitement** : Les outliers sont analys√©s au cas par cas. Certains sont l√©gitimes (ex : tr√®s hauts revenus) et conserv√©s, d'autres sont plafonn√©s (capping) ou supprim√©s.

### 5.2 Gestion des Valeurs Manquantes

**Analyse pr√©alable** :
```python
missing_percent = (df.isnull().sum() / len(df)) * 100
print(missing_percent[missing_percent > 0].sort_values(ascending=False))
```

#### 5.2.1 Strat√©gies d'Imputation

**Pour les variables num√©riques** :

| Variable | Taux de manquants | Strat√©gie | Justification |
|----------|-------------------|-----------|---------------|
| `income` | < 5% | M√©diane | Robuste aux outliers |
| `credit_history_length` | < 10% | Moyenne | Distribution normale |
| `debt_to_income` | > 20% | KNN Imputer | Pr√©serve les relations |

**Pour les variables cat√©gorielles** :

| Variable | Taux de manquants | Strat√©gie | Justification |
|----------|-------------------|-----------|---------------|
| `education` | < 5% | Mode | Valeur la plus fr√©quente |
| `employment_type` | > 10% | Cat√©gorie "Unknown" | Informative en soi |

#### 5.2.2 Imputation Avanc√©e

**KNN Imputer** : Pour les variables avec patterns complexes
```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df_imputed = imputer.fit_transform(df_numeric)
```

**Justification** : KNN pr√©serve les relations entre variables en utilisant les K plus proches voisins pour estimer les valeurs manquantes.

### 5.3 Encodage des Variables Cat√©gorielles

#### 5.3.1 Label Encoding

**Pour les variables ordinales** :
```python
from sklearn.preprocessing import LabelEncoder

# Education : ordinalit√© claire
education_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}
df['education_encoded'] = df['education'].map(education_order)
```

**Justification** : L'ordre est significatif et doit √™tre pr√©serv√©.

#### 5.3.2 One-Hot Encoding

**Pour les variables nominales** :
```python
# Variables sans ordre intrins√®que
df_encoded = pd.get_dummies(df, columns=['industry', 'marital_status'], 
                             drop_first=True)
```

**Justification** : √âvite d'introduire un ordre artificiel. Le param√®tre `drop_first=True` √©vite la multicolin√©arit√©.

#### 5.3.3 Target Encoding

**Pour les variables √† haute cardinalit√©** :
```python
# Si 'job_title' a 100+ cat√©gories
target_mean = df.groupby('job_title')['default'].mean()
df['job_title_encoded'] = df['job_title'].map(target_mean)
```

**Justification** : R√©duit la dimensionnalit√© tout en capturant l'information relative √† la cible.

### 5.4 Normalisation et Standardisation

#### 5.4.1 Standardisation (Z-score)

**Formule** : `z = (x - Œº) / œÉ`

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features = ['income', 'loan_amount', 'age']
df[numerical_features] = scaler.fit_transform(df[numerical_features])
```

**Justification** : N√©cessaire pour les algorithmes sensibles √† l'√©chelle (SVM, R√©gression Logistique, KNN).

#### 5.4.2 Normalisation Min-Max

**Formule** : `x_norm = (x - x_min) / (x_max - x_min)`

**Utilisation** : Pour les features devant rester dans [0, 1], notamment pour les r√©seaux de neurones.

**Choix** : Nous privil√©gions la standardisation pour ce projet car elle est plus robuste aux outliers.

---

## 6. Analyse Exploratoire des Donn√©es (EDA)

### 6.1 Statistiques Descriptives

**R√©sum√© des variables num√©riques** :
```python
df.describe().T
```

**Interpr√©tation attendue** :
- **√Çge moyen** : ~40 ans (population active)
- **Revenu m√©dian** : ~2500-3000‚Ç¨
- **Montant moyen de pr√™t** : ~15000‚Ç¨
- **Taux de d√©faut** : ~10-20% (d√©s√©quilibre typique)

### 6.2 Visualisation des Distributions

#### 6.2.1 Variables Num√©riques

**Histogrammes avec statistiques** :
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Configuration du style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 100

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
numerical_cols = ['age', 'income', 'loan_amount', 'debt_to_income', 
                  'credit_history_length', 'num_credit_lines']

for i, col in enumerate(numerical_cols):
    ax = axes[i//3, i%3]
    
    # Histogramme avec KDE
    df[col].hist(bins=30, ax=ax, edgecolor='black', alpha=0.7, color='steelblue')
    df[col].plot(kind='kde', ax=ax, secondary_y=True, color='red', linewidth=2)
    
    # Statistiques
    mean_val = df[col].mean()
    median_val = df[col].median()
    
    # Lignes verticales pour moyenne et m√©diane
    ax.axvline(mean_val, color='green', linestyle='--', linewidth=2, label=f'Moyenne: {mean_val:.2f}')
    ax.axvline(median_val, color='orange', linestyle='--', linewidth=2, label=f'M√©diane: {median_val:.2f}')
    
    # Labels et titre
    ax.set_title(f'Distribution de {col}', fontsize=12, fontweight='bold')
    ax.set_xlabel(col, fontsize=10)
    ax.set_ylabel('Fr√©quence', fontsize=10)
    ax.legend(loc='upper right', fontsize=8)
    ax.grid(True, alpha=0.3)

plt.suptitle('Analyse des Distributions - Variables Num√©riques', 
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('reports/figures/01_distributions_numeriques.png', dpi=300, bbox_inches='tight')
plt.show()
```

**üìä Graphique attendu** : 6 sous-graphiques avec histogrammes + courbes KDE + lignes de moyenne/m√©diane

**Interpr√©tation** :
- **Revenu** : Distribution asym√©trique (long tail √† droite) ‚Üí n√©cessite transformation log
- **√Çge** : Distribution relativement normale avec pic 30-50 ans
- **Montant du pr√™t** : Distribution multimodale ‚Üí segments de clients diff√©rents
- **Debt_to_income** : Concentration entre 0.2-0.5 avec outliers √† surveiller

#### 6.2.2 Boxplots pour D√©tection des Outliers

```python
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

for i, col in enumerate(numerical_cols):
    ax = axes[i//3, i%3]
    
    # Boxplot avec points individuels
    bp = ax.boxplot(df[col].dropna(), vert=True, patch_artist=True,
                    boxprops=dict(facecolor='lightblue', alpha=0.7),
                    medianprops=dict(color='red', linewidth=2),
                    whiskerprops=dict(color='black', linewidth=1.5),
                    capprops=dict(color='black', linewidth=1.5))
    
    # Ajouter scatter des outliers
    outliers = df[col][((df[col] < df[col].quantile(0.25) - 1.5*(df[col].quantile(0.75)-df[col].quantile(0.25))) | 
                        (df[col] > df[col].quantile(0.75) + 1.5*(df[col].quantile(0.75)-df[col].quantile(0.25))))]
    
    # Statistiques
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    
    # Annotations
    ax.text(0.98, 0.98, f'Q1: {Q1:.2f}\nQ3: {Q3:.2f}\nIQR: {IQR:.2f}\nOutliers: {len(outliers)}',
            transform=ax.transAxes, verticalalignment='top', horizontalalignment='right',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=8)
    
    ax.set_title(f'Boxplot - {col}', fontsize=12, fontweight='bold')
    ax.set_ylabel('Valeur', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

plt.suptitle('D√©tection des Outliers - Analyse par Boxplots', 
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('reports/figures/02_boxplots_outliers.png', dpi=300, bbox_inches='tight')
plt.show()
```

**üìä Graphique attendu** : 6 boxplots avec statistiques Q1, Q3, IQR et nombre d'outliers

**Interpr√©tation** : Les outliers dans `income` et `loan_amount` sont conserv√©s car l√©gitimes (clients fortun√©s, pr√™ts immobiliers). Les outliers extr√™mes dans `num_late_payments` (>10) sont plafonn√©s.

#### 6.2.3 Variables Cat√©gorielles

```python
categorical_cols = ['gender', 'education', 'employment_type', 'marital_status']

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

for i, col in enumerate(categorical_cols):
    ax = axes[i//2, i%2]
    
    # Calcul des valeurs et pourcentages
    value_counts = df[col].value_counts()
    percentages = (value_counts / len(df)) * 100
    
    # Barplot avec couleurs d√©grad√©es
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(value_counts)))
    bars = ax.bar(range(len(value_counts)), value_counts.values, color=colors, 
                   edgecolor='black', linewidth=1.5, alpha=0.8)
    
    # Annotations des valeurs et pourcentages
    for j, (bar, val, pct) in enumerate(zip(bars, value_counts.values, percentages.values)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(val)}\n({pct:.1f}%)',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    # Configuration des axes
    ax.set_xticks(range(len(value_counts)))
    ax.set_xticklabels(value_counts.index, rotation=45, ha='right')
    ax.set_title(f'Distribution de {col}', fontsize=12, fontweight='bold')
    ax.set_ylabel('Nombre de clients', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    
    # Ligne de r√©f√©rence pour la moyenne
    ax.axhline(value_counts.mean(), color='red', linestyle='--', 
               linewidth=2, alpha=0.7, label=f'Moyenne: {value_counts.mean():.0f}')
    ax.legend()

plt.suptitle('Analyse des Variables Cat√©gorielles', 
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('reports/figures/03_distributions_categorielles.png', dpi=300, bbox_inches='tight')
plt.show()
```

**üìä Graphique attendu** : 4 barplots avec valeurs absolues, pourcentages et ligne de moyenne

**Interpr√©tation** :
- **Gender** : Distribution relativement √©quilibr√©e (48% F, 52% M)
- **Education** : Majorit√© Bachelor (42%), suivi de Master (28%)
- **Employment_type** : Dominance Full-time (67%), Self-employed (18%)
- **Marital_status** : Married (55%), Single (30%), Divorced (15%)

### 6.3 Analyse des Corr√©lations

#### 6.3.1 Heatmap de Corr√©lation

```python
plt.figure(figsize=(16, 14))

# Calcul de la matrice de corr√©lation
correlation_matrix = df[numerical_cols + ['default']].corr()

# Cr√©ation du mask pour le triangle sup√©rieur
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Heatmap avec annotations
sns.heatmap(correlation_matrix, 
            mask=mask,
            annot=True, 
            fmt='.2f',
            cmap='RdYlGn',
            center=0, 
            square=True,
            linewidths=1,
            cbar_kws={"shrink": 0.8, "label": "Coefficient de Corr√©lation"},
            vmin=-1, vmax=1)

plt.title('Matrice de Corr√©lation des Variables Num√©riques\n(M√©thode de Pearson)', 
          fontsize=16, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

# Ajouter une note explicative
plt.text(0.5, -0.15, 
         'Note: Rouge = Corr√©lation n√©gative | Vert = Corr√©lation positive | Jaune = Pas de corr√©lation',
         transform=plt.gca().transAxes, ha='center', fontsize=10, style='italic',
         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))

plt.tight_layout()
plt.savefig('reports/figures/04_heatmap_correlation.png', dpi=300, bbox_inches='tight')
plt.show()
```

**üìä Graphique attendu** : Heatmap triangulaire avec gradient de couleurs rouge-jaune-vert et annotations des coefficients

**Interpr√©tation attendue** :
- **Corr√©lation positive forte** : 
  - `loan_amount` ‚Üî `income` (r = 0.68) - Les clients ais√©s demandent des pr√™ts plus importants
  - `age` ‚Üî `credit_history_length` (r = 0.54) - Les personnes √¢g√©es ont un historique plus long
  
- **Corr√©lation mod√©r√©e** :
  - `num_credit_lines` ‚Üî `income` (r = 0.42) - Plus de revenus = plus de lignes de cr√©dit
  
- **Corr√©lation n√©gative** :
  - `num_late_payments` ‚Üî `income` (r = -0.35) - Les revenus √©lev√©s ont moins de retards
  - `debt_to_income` ‚Üî `credit_score` (r = -0.48) - Plus d'endettement = score plus faible

**Point d'attention** : Pas de multicolin√©arit√© pr√©occupante (r > 0.9) d√©tect√©e.

#### 6.3.2 Corr√©lation avec la Variable Cible

```python
# Calcul et tri des corr√©lations avec la cible
target_corr = df.corr()['default'].drop('default').sort_values(ascending=False)

# Cr√©ation de la figure
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))

# Graphique 1: Barplot horizontal des corr√©lations
colors = ['green' if x < 0 else 'red' for x in target_corr.values]
bars = ax1.barh(range(len(target_corr)), target_corr.values, color=colors, alpha=0.7, edgecolor='black')

# Annotations
for i, (bar, val) in enumerate(zip(bars, target_corr.values)):
    ax1.text(val, i, f' {val:.3f}', va='center', ha='left' if val > 0 else 'right', 
             fontweight='bold', fontsize=9)

ax1.set_yticks(range(len(target_corr)))
ax1.set_yticklabels(target_corr.index)
ax1.axvline(x=0, color='black', linestyle='-', linewidth=1.5)
ax1.set_xlabel('Coefficient de Corr√©lation', fontsize=12, fontweight='bold')
ax1.set_title('Corr√©lation des Features avec le D√©faut de Paiement', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='x')

# Graphique 2: Top 10 des corr√©lations absolues
top_10_abs = target_corr.abs().sort_values(ascending=False).head(10)
colors_top10 = ['red' if target_corr[feat] > 0 else 'green' for feat in top_10_abs.index]

ax2.bar(range(len(top_10_abs)), top_10_abs.values, color=colors_top10, alpha=0.7, edgecolor='black')
ax2.set_xticks(range(len(top_10_abs)))
ax2.set_xticklabels(top_10_abs.index, rotation=45, ha='right')
ax2.set_ylabel('Corr√©lation Absolue', fontsize=12, fontweight='bold')
ax2.set_title('Top 10 Variables Pr√©dictives (Corr√©lation Absolue)', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# Annotations des valeurs
for i, val in enumerate(top_10_abs.values):
    ax2.text(i, val, f'{val:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)

# L√©gende
from matplotlib.patches import Patch
legend_elements = [Patch(facecolor='red', alpha=0.7, label='Corr√©lation positive (‚Üë risque)'),
                   Patch(facecolor='green', alpha=0.7, label='Corr√©lation n√©gative (‚Üì risque)')]
ax2.legend(handles=legend_elements, loc='upper right')

plt.tight_layout()
plt.savefig('reports/figures/05_correlation_target.png', dpi=300, bbox_inches='tight')
plt.show()

# Affichage des top features
print("\n" + "="*60)
print("TOP 5 VARIABLES PR√âDICTIVES DU D√âFAUT DE PAIEMENT")
print("="*60)
for i, (feat, corr) in enumerate(target_corr.abs().sort_values(ascending=False).head(5).items(), 1):
    direction = "‚Üë AUGMENTE" if target_corr[feat] > 0 else "‚Üì DIMINUE"
    print(f"{i}. {feat:30s} | r = {target_corr[feat]:+.3f} | {direction} le risque")
print("="*60)
```

**üìä Graphiques attendus** : 
1. Barplot horizontal de toutes les corr√©lations (rouge = positive, vert = n√©gative)
2. Barplot du top 10 des corr√©lations absolues avec valeurs annot√©es

**Variables pr√©dictives attendues** :
1. üî¥ **num_late_payments** (r ‚âà +0.62) - Forte corr√©lation positive
2. üî¥ **debt_to_income** (r ‚âà +0.48) - Corr√©lation positive mod√©r√©e
3. üü¢ **income** (r ‚âà -0.41) - Corr√©lation n√©gative mod√©r√©e
4. üü¢ **credit_history_length** (r ‚âà -0.38) - Corr√©lation n√©gative mod√©r√©e
5. üî¥ **loan_to_income_ratio** (r ‚âà +0.35) - Variable engineered pertinente

**Analyse** : Les retards de paiement ant√©rieurs sont le meilleur pr√©dicteur individuel du risque de d√©faut, confirmant l'importance de l'historique comportemental.

### 6.4 Feature Engineering

**Cr√©ation de nouvelles variables pertinentes** :

#### 6.4.1 Ratios Financiers

```python
# Ratio mensualit√©/revenu
df['payment_to_income'] = (df['loan_amount'] / df['loan_term']) / df['income']

# Capacit√© d'√©pargne
df['savings_capacity'] = df['income'] - (df['income'] * df['debt_to_income'])

# Ratio cr√©dit utilis√©
df['credit_usage_ratio'] = df['num_credit_lines'] / df['credit_history_length']
```

**Justification** : Ces ratios capturent mieux la capacit√© de remboursement qu'une variable isol√©e.

#### 6.4.2 Variables Binaires

```python
# Client senior (> 60 ans)
df['is_senior'] = (df['age'] > 60).astype(int)

# Haut revenu (top 25%)
df['high_income'] = (df['income'] > df['income'].quantile(0.75)).astype(int)

# Historique cr√©dit long
df['long_credit_history'] = (df['credit_history_length'] > 10).astype(int)
```

**Justification** : Capture des seuils non-lin√©aires importants pour la d√©cision.

#### 6.4.3 Variables d'Interaction

```python
# Interaction √¢ge √ó revenu
df['age_income'] = df['age'] * df['income']

# Interaction √©ducation √ó emploi
df['edu_emp'] = df['education_encoded'] * df['employment_type_encoded']
```

**Justification** : Capture les effets combin√©s de plusieurs variables.

#### 6.4.4 Variables Agr√©g√©es

```python
# Score de risque composite
df['risk_score'] = (
    df['num_late_payments'] * 2 + 
    df['debt_to_income'] * 10 +
    (1 / (df['income'] + 1)) * 1000
)
```

**Justification** : Combine plusieurs indicateurs de risque en une m√©trique unique.

---

## 7. Mod√©lisation Machine Learning

### 7.1 S√©paration Train/Test

**Strat√©gie** : Split 80/20 avec stratification pour pr√©server le ratio de classes.

```python
from sklearn.model_selection import train_test_split

X = df.drop('default', axis=1)
y = df['default']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Train set: {X_train.shape}")
print(f"Test set: {X_test.shape}")
print(f"Distribution train: {y_train.value_counts(normalize=True)}")
print(f"Distribution test: {y_test.value_counts(normalize=True)}")
```

**Justification** :
- 80/20 offre suffisamment de donn√©es d'entra√Ænement tout en conservant un test robuste
- Stratification garantit la m√™me proportion de classes dans train et test
- `random_state=42` assure la reproductibilit√©

### 7.2 S√©lection des Algorithmes

Nous comparons trois familles d'algorithmes aux caract√©ristiques compl√©mentaires :

#### 7.2.1 R√©gression Logistique

**Caract√©ristiques** :
- Algorithme lin√©aire simple et interpr√©table
- Rapide √† entra√Æner
- Baseline de r√©f√©rence

**Impl√©mentation** :
```python
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(random_state=42, max_iter=1000)
logreg.fit(X_train, y_train)
```

**Avantages** : Coefficients interpr√©tables, probabilit√©s calibr√©es  
**Inconv√©nients** : Suppose une relation lin√©aire

#### 7.2.2 Random Forest

**Caract√©ristiques** :
- Ensemble d'arbres de d√©cision
- G√®re bien les non-lin√©arit√©s
- Robuste aux outliers

**Impl√©mentation** :
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
```

**Avantages** : Peu de preprocessing requis, importance des features  
**Inconv√©nients** : Peut overfitter, moins interpr√©table

#### 7.2.3 XGBoost (Gradient Boosting)

**Caract√©ristiques** :
- √âtat de l'art pour donn√©es tabulaires
- Boosting it√©ratif
- G√®re nativement les valeurs manquantes

**Impl√©mentation** :
```python
import xgboost as xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    eval_metric='logloss'
)
xgb_model.fit(X_train, y_train)
```

**Avantages** : Performances exceptionnelles, r√©gularisation int√©gr√©e  
**Inconv√©nients** : Temps de calcul plus long, n√©cessite tuning

**Justification du choix** : Ces trois algorithmes offrent une comparaison compl√®te entre approche lin√©aire, bagging et boosting.

### 7.3 Validation Crois√©e

**Strat√©gie** : K-Fold Cross-Validation stratifi√©e avec k=5

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Pour chaque mod√®le
for name, model in [('LogReg', logreg), ('RF', rf), ('XGB', xgb_model)]:
    cv_scores = cross_val_score(
        model, X_train, y_train, 
        cv=skf, 
        scoring='roc_auc',
        n_jobs=-1
    )
    print(f"{name} - CV AUC-ROC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
```

**Justification** :
- K=5 offre un bon compromis entre variance et biais
- Stratification maintient la distribution des classes dans chaque fold
- AUC-ROC est la m√©trique principale pour le d√©s√©quilibre de classes

### 7.4 Optimisation des Hyperparam√®tres

#### 7.4.1 Grid Search pour Random Forest

```python
from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

grid_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_rf,
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)
grid_rf.fit(X_train, y_train)

print(f"Meilleurs param√®tres RF: {grid_rf.best_params_}")
print(f"Meilleur score CV: {grid_rf.best_score_:.4f}")
```

#### 7.4.2 Randomized Search pour XGBoost

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

param_dist_xgb = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}

random_xgb = RandomizedSearchCV(
    xgb.XGBClassifier(random_state=42),
    param_dist_xgb,
    n_iter=50,
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1,
    random_state=42
)
random_xgb.fit(X_train, y_train)

print(f"Meilleurs param√®tres XGB: {random_xgb.best_params_}")
print(f"Meilleur score CV: {random_xgb.best_score_:.4f}")
```

**Justification** :
- GridSearch pour RF : espace de recherche raisonnable
- RandomizedSearch pour XGB : espace de recherche vaste, plus efficace
- N_iter=50 offre une bonne exploration sans temps excessif

---

## 8. R√©sultats et Discussion

### 8.1 M√©triques de Performance

**√âvaluation sur le jeu de test** :

```python
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                              f1_score, roc_auc_score, classification_report,
                              confusion_matrix, roc_curve)

def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'AUC-ROC': roc_auc_score(y_test, y_pred_proba)
    }
    
    print(f"\n=== {model_name} ===")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")
    
    return metrics, y_pred, y_pred_proba
```

**Tableau r√©capitulatif des performances** :

| Mod√®le | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|--------|----------|-----------|--------|----------|---------|
| R√©gression Logistique | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] |
| Random Forest | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] |
| XGBoost | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] |

### 8.2 Comparaison des Mod√®les

#### 8.2.1 Courbes ROC

```python
plt.figure(figsize=(12, 10))

models_dict = {
    'R√©gression Logistique': logreg_best,
    'Random Forest': rf_best,
    'XGBoost': xgb_best
}

colors = {'R√©gression Logistique': 'blue', 'Random Forest': 'green', 'XGBoost': 'red'}
linestyles = {'R√©gression Logistique': '-', 'Random Forest': '--', 'XGBoost': '-.'}

for name, model in models_dict.items():
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', 
             color=colors[name], linestyle=linestyles[name], linewidth=2.5)
    
    # Marquer le point optimal (Youden's J statistic)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    plt.plot(fpr[optimal_idx], tpr[optimal_idx], marker='o', markersize=8, 
             color=colors[name], markeredgecolor='black', markeredgewidth=1.5)

# Ligne de r√©f√©rence (classificateur al√©atoire)
plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Classificateur Al√©atoire (AUC = 0.500)', alpha=0.5)

# Configuration du graphique
plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12, fontweight='bold')
plt.ylabel('Taux de Vrais Positifs (TPR / Recall)', fontsize=12, fontweight='bold')
plt.title('Courbes ROC - Comparaison des Mod√®les de Credit Scoring', 
          fontsize=14, fontweight='bold', pad=15)
plt.legend(loc='lower right', fontsize=11, framealpha=0.9)
plt.grid(True, alpha=0.3, linestyle='--')

# Zone d'excellence
plt.fill_between([0, 0.2], [0.8, 1], alpha=0.1, color='green', label='Zone d\'Excellence')

# Annotations
plt.text(0.6, 0.3, 'AUC > 0.8 : Bon mod√®le\nAUC > 0.9 : Excellent mod√®le', 
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),
         fontsize=10, style='italic')

plt.xlim([-0.02, 1.02])
plt.ylim([-0.02, 1.02])
plt.tight_layout()
plt.savefig('reports/figures/06_roc_curves_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
```

**üìä Graphique attendu** : Courbes ROC superpos√©es avec points optimaux marqu√©s et zone d'excellence

**Interpr√©tation attendue** :
- **XGBoost** : Courbe la plus proche du coin sup√©rieur gauche (AUC ‚âà 0.88)
- **Random Forest** : Performance l√©g√®rement inf√©rieure (AUC ‚âà 0.85)
- **R√©gression Logistique** : Baseline solide (AUC ‚âà 0.79)
- Les points marqu√©s indiquent le seuil optimal pour chaque mod√®le
- √âcart significatif avec le classificateur al√©atoire confirme la valeur pr√©dictive

#### 8.2.2 Visualisation Comparative des M√©triques

```python
# Pr√©paration des donn√©es de m√©triques
metrics_df = pd.DataFrame({
    'R√©gression Logistique': metrics_logreg,
    'Random Forest': metrics_rf,
    'XGBoost': metrics_xgb
})

# Cr√©ation de la figure avec plusieurs subplots
fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)

# 1. Barplot group√© des m√©triques
ax1 = fig.add_subplot(gs[0, :])
x = np.arange(len(metrics_df.columns))
width = 0.15
metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
colors_metrics = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']

for i, metric in enumerate(metrics_list):
    values = [metrics_df[col][metric] for col in metrics_df.columns]
    bars = ax1.bar(x + i*width, values, width, label=metric, color=colors_metrics[i], 
                   alpha=0.8, edgecolor='black', linewidth=1.5)
    
    # Annotations
    for j, (bar, val) in enumerate(zip(bars, values)):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{val:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')

ax1.set_xlabel('Mod√®les', fontsize=12, fontweight='bold')
ax1.set_ylabel('Score', fontsize=12, fontweight='bold')
ax1.set_title('Comparaison Compl√®te des M√©triques par Mod√®le', fontsize=14, fontweight='bold')
ax1.set_xticks(x + width * 2)
ax1.set_xticklabels(metrics_df.columns)
ax1.legend(loc='upper left', ncol=5, fontsize=10)
ax1.set_ylim([0, 1.1])
ax1.grid(True, alpha=0.3, axis='y')
ax1.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Seuil Excellence (0.8)')

# 2. Radar Chart (Spider Plot)
ax2 = fig.add_subplot(gs[1, 0], projection='polar')

categories = metrics_list
N = len(categories)
angles = [n / float(N) * 2 * np.pi for n in range(N)]
angles += angles[:1]

for model_name, color in zip(metrics_df.columns, ['blue', 'green', 'red']):
    values = metrics_df[model_name].values.tolist()
    values += values[:1]
    ax2.plot(angles, values, 'o-', linewidth=2, label=model_name, color=color, alpha=0.7)
    ax2.fill(angles, values, alpha=0.15, color=color)

ax2.set_xticks(angles[:-1])
ax2.set_xticklabels(categories, fontsize=10)
ax2.set_ylim(0, 1)
ax2.set_title('Comparaison Radar - Performance Globale', fontsize=12, fontweight='bold', pad=20)
ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
ax2.grid(True)

# 3. Heatmap des performances
ax3 = fig.add_subplot(gs[1, 1])
sns.heatmap(metrics_df, annot=True, fmt='.3f', cmap='RdYlGn', center=0.5, 
            vmin=0, vmax=1, cbar_kws={'label': 'Score'}, ax=ax3,
            linewidths=2, linecolor='black')
ax3.set_title('Heatmap des Performances', fontsize=12, fontweight='bold')
ax3.set_xlabel('Mod√®les', fontsize=10, fontweight='bold')
ax3.set_ylabel('M√©triques', fontsize=10, fontweight='bold')

# 4. Temps d'entra√Ænement vs Performance
ax4 = fig.add_subplot(gs[2, 0])
# Donn√©es simul√©es (√† remplacer par vos vraies mesures)
training_times = [2.3, 45.7, 123.4]  # secondes
auc_scores = [metrics_df[col]['AUC-ROC'] for col in metrics_df.columns]

scatter = ax4.scatter(training_times, auc_scores, s=300, alpha=0.6, 
                     c=['blue', 'green', 'red'], edgecolors='black', linewidths=2)

for i, model in enumerate(metrics_df.columns):
    ax4.annotate(model, (training_times[i], auc_scores[i]), 
                fontsize=10, ha='center', fontweight='bold')

ax4.set_xlabel('Temps d\'Entra√Ænement (secondes)', fontsize=12, fontweight='bold')
ax4.set_ylabel('AUC-ROC Score', fontsize=12, fontweight='bold')
ax4.set_title('Trade-off Performance vs Temps de Calcul', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3)
ax4.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Seuil Minimum')
ax4.legend()

# 5. Pr√©cision vs Recall (Trade-off)
ax5 = fig.add_subplot(gs[2, 1])
precisions = [metrics_df[col]['Precision'] for col in metrics_df.columns]
recalls = [metrics_df[col]['Recall'] for col in metrics_df.columns]

for i, model in enumerate(metrics_df.columns):
    ax5.scatter(recalls[i], precisions[i], s=300, alpha=0.6,
               c=['blue', 'green', 'red'][i], edgecolors='black', linewidths=2)
    ax5.annotate(model, (recalls[i], precisions[i]), 
                fontsize=10, ha='center', fontweight='bold',
                xytext=(10, 10), textcoords='offset points')

# Lignes de r√©f√©rence
ax5.axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, label='Precision = 0.7')
ax5.axvline(x=0.7, color='purple', linestyle='--', alpha=0.5, label='Recall = 0.7')

ax5.set_xlabel('Recall (Sensibilit√©)', fontsize=12, fontweight='bold')
ax5.set_ylabel('Precision', fontsize=12, fontweight='bold')
ax5.set_title('Trade-off Precision vs Recall', fontsize=12, fontweight='bold')
ax5.set_xlim([0.5, 1.0])
ax5.set_ylim([0.5, 1.0])
ax5.grid(True, alpha=0.3)
ax5.legend()

plt.savefig('reports/figures/07_metrics_comprehensive_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
```

**üìä Graphiques attendus** : 
1. Barplot group√© avec toutes les m√©triques annot√©es
2. Radar chart montrant le profil de performance de chaque mod√®le
3. Heatmap color√©e des scores
4. Scatter plot performance vs temps de calcul
5. Trade-off Precision-Recall

**Analyse** : Cette visualisation multi-facettes permet de comparer les mod√®les selon plusieurs dimensions simultan√©ment.

### 8.3 Analyse des Erreurs

#### 8.3.1 Matrice de Confusion

```python
from sklearn.metrics import ConfusionMatrixDisplay

fig, axes = plt.subplots(2, 3, figsize=(20, 14))

# Ligne 1: Matrices de confusion
for idx, (name, model) in enumerate([('R√©gression Logistique', logreg_best), 
                                      ('Random Forest', rf_best), 
                                      ('XGBoost', xgb_best)]):
    ax = axes[0, idx]
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    
    # Calcul des pourcentages
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # Affichage avec valeurs absolues et pourcentages
    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', ax=ax, 
                cbar_kws={'label': 'Nombre de pr√©dictions'},
                linewidths=2, linecolor='black')
    
    # Annotations personnalis√©es
    for i in range(2):
        for j in range(2):
            text = ax.text(j+0.5, i+0.5, 
                          f'{cm[i, j]}\n({cm_percent[i, j]:.1f}%)',
                          ha="center", va="center", color="white" if cm[i, j] > cm.max()/2 else "black",
                          fontsize=14, fontweight='bold')
    
    ax.set_title(f'Matrice de Confusion - {name}', fontsize=12, fontweight='bold')
    ax.set_xlabel('Pr√©diction', fontsize=11, fontweight='bold')
    ax.set_ylabel('R√©alit√©', fontsize=11, fontweight='bold')
    ax.set_xticklabels(['Bon Payeur (0)', 'D√©faut (1)'], fontsize=10)
    ax.set_yticklabels(['Bon Payeur (0)', 'D√©faut (1)'], fontsize=10, rotation=0)

# Ligne 2: Analyse d√©taill√©e des erreurs
for idx, (name, model) in enumerate([('R√©gression Logistique', logreg_best), 
                                      ('Random Forest', rf_best), 
                                      ('XGBoost', xgb_best)]):
    ax = axes[1, idx]
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    
    TN, FP, FN, TP = cm.ravel()
    
    # Calcul des co√ªts (hypoth√®se)
    cost_FN = 10000  # Perte par faux n√©gatif
    cost_FP = 500    # Manque √† gagner par faux positif
    total_cost = (FN * cost_FN) + (FP * cost_FP)
    
    # Barplot des types d'erreurs
    categories = ['VN\n(Correct)', 'FP\n(Erreur)', 'FN\n(Erreur)', 'VP\n(Correct)']
    values = [TN, FP, FN, TP]
    colors_bars = ['green', 'orange', 'red', 'green']
    
    bars = ax.bar(categories, values, color=colors_bars, alpha=0.7, edgecolor='black', linewidth=2)
    
    # Annotations
    for bar, val in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(val)}', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    ax.set_title(f'Analyse des Erreurs - {name}\nCo√ªt Total: {total_cost:,.0f}‚Ç¨', 
                fontsize=11, fontweight='bold')
    ax.set_ylabel('Nombre de cas', fontsize=10, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Ajouter les taux
    ax.text(0.95, 0.95, f'Taux FP: {(FP/(FP+TN)*100):.1f}%\nTaux FN: {(FN/(FN+TP)*100):.1f}%',
            transform=ax.transAxes, verticalalignment='top', horizontalalignment='right',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8), fontsize=9)

plt.suptitle('Analyse Compl√®te des Matrices de Confusion et Impact Business', 
             fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig('reports/figures/08_confusion_matrices_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# Tableau r√©capitulatif
print("\n" + "="*80)
print("ANALYSE DES ERREURS ET IMPACT BUSINESS")
print("="*80)
print(f"{'Mod√®le':<25} | {'FN':<8} | {'FP':<8} | {'Co√ªt Total (‚Ç¨)':<15} | {'Taux FN':<10}")
print("-"*80)
for name, model in [('R√©gression Logistique', logreg_best), 
                    ('Random Forest', rf_best), 
                    ('XGBoost', xgb_best)]:
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    TN, FP, FN, TP = cm.ravel()
    cost = (FN * 10000) + (FP * 500)
    fn_rate = FN/(FN+TP)*100
    print(f"{name:<25} | {FN:<8} | {FP:<8} | {cost:<15,.0f} | {fn_rate:<10.2f}%")
print("="*80)
```

**üìä Graphiques attendus** : 
- Ligne 1: 3 matrices de confusion avec valeurs absolues et pourcentages
- Ligne 2: 3 barplots des types de pr√©dictions avec co√ªts business

**Analyse des erreurs** :

| Type d'erreur | D√©finition | Impact Business | Priorit√© |
|---------------|------------|-----------------|----------|
| **VN (Vrai N√©gatif)** | Bon payeur correctement identifi√© | ‚úÖ Cr√©dit accord√© √† bon escient | Positif |
| **VP (Vrai Positif)** | D√©faut correctement pr√©dit | ‚úÖ Cr√©dit refus√© √©vite la perte | Positif |
| **FP (Faux Positif)** | Bon payeur class√© comme risqu√© | ‚ö†Ô∏è Manque √† gagner ~500‚Ç¨ | Mod√©r√© |
| **FN (Faux N√©gatif)** | Mauvais payeur accept√© | üî¥ Perte directe ~10 000‚Ç¨ | **CRITIQUE** |

**Objectif prioritaire** : Minimiser les FN m√™me si cela augmente l√©g√®rement les FP.

#### 8.3.2 Analyse des Cas Mal Class√©s

```python
# Identifier les faux n√©gatifs (les plus co√ªteux)
false_negatives = X_test[(y_test == 1) & (y_pred == 0)]

print(f"Nombre de faux n√©gatifs: {len(false_negatives)}")
print("\nCaract√©ristiques moyennes des faux n√©gatifs:")
print(false_negatives.describe())

# Comparer avec les vrais positifs
true_positives = X_test[(y_test == 1) & (y_pred == 1)]
comparison = pd.DataFrame({
    'Faux N√©gatifs': false_negatives.mean(),
    'Vrais Positifs': true_positives.mean()
})
print("\nComparaison:")
print(comparison)
```

**Insights attendus** :
- Les faux n√©gatifs ont souvent des caract√©ristiques "borderline"
- Variables discriminantes insuffisamment captur√©es
- N√©cessit√© de features engineering additionnel ou ajustement du seuil

### 8.4 Interpr√©tabilit√© du Mod√®le

#### 8.4.1 Importance des Features (Random Forest & XGBoost)

```python
# Pr√©paration des donn√©es d'importance
feature_importance_rf = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_best.feature_importances_
}).sort_values('Importance', ascending=False)

feature_importance_xgb = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': xgb_best.feature_importances_
}).sort_values('Importance', ascending=False)

# Cr√©ation de la figure
fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)

# 1. Random Forest - Top 15 features
ax1 = fig.add_subplot(gs[0, 0])
top15_rf = feature_importance_rf.head(15)
colors_rf = plt.cm.Blues(np.linspace(0.4, 0.9, len(top15_rf)))
bars1 = ax1.barh(range(len(top15_rf)), top15_rf['Importance'], color=colors_rf, 
                 edgecolor='black', linewidth=1.5)

# Annotations
for i, (bar, val) in enumerate(zip(bars1, top15_rf['Importance'])):
    ax1.text(val, i, f' {val:.4f}', va='center', fontsize=9, fontweight='bold')

ax1.set_yticks(range(len(top15_rf)))
ax1.set_yticklabels(top15_rf['Feature'])
ax1.invert_yaxis()
ax1.set_xlabel('Importance (Gini)', fontsize=11, fontweight='bold')
ax1.set_title('Top 15 Features - Random Forest', fontsize=13, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='x')

# 2. XGBoost - Top 15 features
ax2 = fig.add_subplot(gs[0, 1])
top15_xgb = feature_importance_xgb.head(15)
colors_xgb = plt.cm.Oranges(np.linspace(0.4, 0.9, len(top15_xgb)))
bars2 = ax2.barh(range(len(top15_xgb)), top15_xgb['Importance'], color=colors_xgb, 
                 edgecolor='black', linewidth=1.5)

# Annotations
for i, (bar, val) in enumerate(zip(bars2, top15_xgb['Importance'])):
    ax2.text(val, i, f' {val:.4f}', va='center', fontsize=9, fontweight='bold')

ax2.set_yticks(range(len(top15_xgb)))
ax2.set_yticklabels(top15_xgb['Feature'])
ax2.invert_yaxis()
ax2.set_xlabel('Importance (Gain)', fontsize=11, fontweight='bold')
ax2.set_title('Top 15 Features - XGBoost', fontsize=13, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='x')

# 3. Comparaison c√¥te √† c√¥te des Top 10
ax3 = fig.add_subplot(gs[1, :])
top10_common = list(set(feature_importance_rf.head(10)['Feature']) | 
                    set(feature_importance_xgb.head(10)['Feature']))[:12]

rf_values = [feature_importance_rf[feature_importance_rf['Feature'] == feat]['Importance'].values[0] 
             if feat in feature_importance_rf['Feature'].values else 0 for feat in top10_common]
xgb_values = [feature_importance_xgb[feature_importance_xgb['Feature'] == feat]['Importance'].values[0] 
              if feat in feature_importance_xgb['Feature'].values else 0 for feat in top10_common]

x = np.arange(len(top10_common))
width = 0.35

bars_rf = ax3.bar(x - width/2, rf_values, width, label='Random Forest', 
                  color='steelblue', alpha=0.8, edgecolor='black', linewidth=1.5)
bars_xgb = ax3.bar(x + width/2, xgb_values, width, label='XGBoost', 
                   color='coral', alpha=0.8, edgecolor='black', linewidth=1.5)

# Annotations
for bars in [bars_rf, bars_xgb]:
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')

ax3.set_xlabel('Features', fontsize=12, fontweight='bold')
ax3.set_ylabel('Importance', fontsize=12, fontweight='bold')
ax3.set_title('Comparaison de l\'Importance des Features : RF vs XGBoost', 
              fontsize=14, fontweight='bold')
ax3.set_xticks(x)
ax3.set_xticklabels(top10_common, rotation=45, ha='right')
ax3.legend(fontsize=11, loc='upper right')
ax3.grid(True, alpha=0.3, axis='y')

plt.savefig('reports/figures/09_feature_importance_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

# Tableau r√©capitulatif
print("\n" + "="*70)
print("TOP 10 FEATURES LES PLUS IMPORTANTES (Consensus RF + XGB)")
print("="*70)
print(f"{'Rang':<6} | {'Feature':<35} | {'RF':<10} | {'XGB':<10}")
print("-"*70)

# Calculer le score moyen normalis√©
rf_norm = feature_importance_rf.copy()
rf_norm['Importance'] = rf_norm['Importance'] / rf_norm['Importance'].max()
xgb_norm = feature_importance_xgb.copy()
xgb_norm['Importance'] = xgb_norm['Importance'] / xgb_norm['Importance'].max()

# Fusion et moyenne
merged = rf_norm.merge(xgb_norm, on='Feature', suffixes=('_rf', '_xgb'))
merged['avg_importance'] = (merged['Importance_rf'] + merged['Importance_xgb']) / 2
merged = merged.sort_values('avg_importance', ascending=False).head(10)

for i, row in enumerate(merged.itertuples(), 1):
    print(f"{i:<6} | {row.Feature:<35} | {row.Importance_rf:<10.4f} | {row.Importance_xgb:<10.4f}")
print("="*70)
```

**üìä Graphiques attendus** : 
1. Barplot horizontal RF avec top 15 features (bleu)
2. Barplot horizontal XGB avec top 15 features (orange)
3. Comparaison c√¥te √† c√¥te des features communes (barplot group√©)

**Interpr√©tation attendue** :
- **num_late_payments** : Feature #1 dans les deux mod√®les (importance ‚âà 0.18-0.22)
- **debt_to_income** : Feature #2 (importance ‚âà 0.15-0.18)
- **income** : Feature #3 (importance ‚âà 0.12-0.14)
- **payment_to_income** (engineered) : Forte importance confirme la valeur du feature engineering
- **Convergence** entre RF et XGB valide la robustesse des features identifi√©es

#### 8.4.2 SHAP Values (SHapley Additive exPlanations)

```python
import shap

# Cr√©er l'explainer pour XGBoost
explainer = shap.TreeExplainer(xgb_best)
shap_values = explainer.shap_values(X_test)

# Figure compl√®te avec 4 visualisations SHAP
fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 2, hspace=0.4, wspace=0.3)

# 1. Summary Plot - Bar (Impact moyen)
ax1 = fig.add_subplot(gs[0, :])
shap.summary_plot(shap_values, X_test, plot_type="bar", show=False, max_display=15)
plt.title('Impact Moyen des Features sur les Pr√©dictions (SHAP Values)', 
          fontsize=14, fontweight='bold', pad=15)
plt.xlabel('Impact Moyen Absolu (SHAP)', fontsize=12, fontweight='bold')

# 2. Summary Plot - Dot (Distribution d√©taill√©e)
ax2 = fig.add_subplot(gs[1, :])
shap.summary_plot(shap_values, X_test, show=False, max_display=15)
plt.title('Distribution des SHAP Values par Feature', fontsize=14, fontweight='bold', pad=15)
plt.xlabel('Impact sur la Pr√©diction (SHAP Value)', fontsize=12, fontweight='bold')

# Note explicative
fig.text(0.5, 0.33, 
         'üî¥ Rouge = Valeur √©lev√©e de la feature | üîµ Bleu = Valeur faible | '
         'Position droite = Augmente probabilit√© de d√©faut',
         ha='center', fontsize=11, style='italic',
         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))

# 3. Dependence Plot - Feature principale
ax3 = fig.add_subplot(gs[2, 0])
top_feature = feature_importance_xgb.iloc[0]['Feature']
shap.dependence_plot(top_feature, shap_values, X_test, show=False, ax=ax3)
ax3.set_title(f'Dependence Plot - {top_feature}', fontsize=12, fontweight='bold')

# 4. Dependence Plot - 2√®me feature
ax4 = fig.add_subplot(gs[2, 1])
second_feature = feature_importance_xgb.iloc[1]['Feature']
shap.dependence_plot(second_feature, shap_values, X_test, show=False, ax=ax4)
ax4.set_title(f'Dependence Plot - {second_feature}', fontsize=12, fontweight='bold')

plt.savefig('reports/figures/10_shap_analysis_global.png', dpi=300, bbox_inches='tight')
plt.show()
```

**üìä Graphiques attendus** : 
1. Bar plot de l'impact moyen absolu (15 features)
2. Beeswarm plot montrant la distribution compl√®te des SHAP values
3. Dependence plot de la feature #1
4. Dependence plot de la feature #2

**Interpr√©tation** :
- **Rouge** (valeurs √©lev√©es) : Effet sur la probabilit√© de d√©faut
- **Bleu** (valeurs faibles) : Impact inverse
- **Position horizontale** : Magnitude de l'impact (droite = augmente risque)

**Exemple d'analyse** : 
- `num_late_payments` √©lev√© (rouge) ‚Üí forte augmentation du risque (SHAP > 0)
- `income` √©lev√© (rouge) ‚Üí diminution du risque (SHAP < 0)

---

#### 8.4.3 Explication d'une Pr√©diction Individuelle (SHAP)

```python
# S√©lectionner deux clients : un bon payeur et un mauvais payeur
good_client_idx = np.where(y_test == 0)[0][0]  # Premier bon payeur
bad_client_idx = np.where(y_test == 1)[0][0]   # Premier mauvais payeur

fig, axes = plt.subplots(2, 2, figsize=(20, 14))

# Client 1 (Bon Payeur) - Force Plot converti en matplotlib
ax1 = axes[0, 0]
shap.force_plot(explainer.expected_value, 
                shap_values[good_client_idx], 
                X_test.iloc[[good_client_idx]],
                matplotlib=True,
                show=False)
plt.title(f'Analyse SHAP - Client #{good_client_idx} (BON PAYEUR - Classe R√©elle: 0)', 
          fontsize=12, fontweight='bold')

# Client 2 (Mauvais Payeur) - Force Plot
ax2 = axes[0, 1]
shap.force_plot(explainer.expected_value, 
                shap_values[bad_client_idx], 
                X_test.iloc[[bad_client_idx]],
                matplotlib=True,
                show=False)
plt.title(f'Analyse SHAP - Client #{bad_client_idx} (MAUVAIS PAYEUR - Classe R√©elle: 1)', 
          fontsize=12, fontweight='bold')

# Client 1 - Waterfall Plot
ax3 = axes[1, 0]
shap.waterfall_plot(shap.Explanation(values=shap_values[good_client_idx], 
                                     base_values=explainer.expected_value, 
                                     data=X_test.iloc[good_client_idx].values,
                                     feature_names=X_test.columns.tolist()),
                   max_display=12, show=False)
plt.title(f'Waterfall - Client #{good_client_idx}', fontsize=12, fontweight='bold')

# Client 2 - Waterfall Plot
ax4 = axes[1, 1]
shap.waterfall_plot(shap.Explanation(values=shap_values[bad_client_idx], 
                                     base_values=explainer.expected_value, 
                                     data=X_test.iloc[bad_client_idx].values,
                                     feature_names=X_test.columns.tolist()),
                   max_display=12, show=False)
plt.title(f'Waterfall - Client #{bad_client_idx}', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('reports/figures/11_shap_individual_explanations.png', dpi=300, bbox_inches='tight')
plt.show()

# Affichage textuel des d√©tails
print("\n" + "="*80)
print(f"EXPLICATION D√âTAILL√âE - CLIENT #{good_client_idx} (BON PAYEUR)")
print("="*80)
print(f"Probabilit√© de d√©faut pr√©dite: {xgb_best.predict_proba(X_test.iloc[[good_client_idx]])[:, 1][0]:.2%}")
print(f"Classe pr√©dite: {xgb_best.predict(X_test.iloc[[good_client_idx]])[0]}")
print(f"Classe r√©elle: {y_test.iloc[good_client_idx]}")
print("\nTop 5 facteurs r√©duisant le risque:")
top_negative = pd.DataFrame({
    'Feature': X_test.columns,
    'SHAP': shap_values[good_client_idx],
    'Valeur': X_test.iloc[good_client_idx].values
}).sort_values('SHAP').head(5)
for i, row in enumerate(top_negative.itertuples(), 1):
    print(f"{i}. {row.Feature:<30s} = {row.Valeur:>10.2f} | Impact: {row.SHAP:>8.4f}")

print("\n" + "="*80)
print(f"EXPLICATION D√âTAILL√âE - CLIENT #{bad_client_idx} (MAUVAIS PAYEUR)")
print("="*80)
print(f"Probabilit√© de d√©faut pr√©dite: {xgb_best.predict_proba(X_test.iloc[[bad_client_idx]])[:, 1][0]:.2%}")
print(f"Classe pr√©dite: {xgb_best.predict(X_test.iloc[[bad_client_idx]])[0]}")
print(f"Classe r√©elle: {y_test.iloc[bad_client_idx]}")
print("\nTop 5 facteurs augmentant le risque:")
top_positive = pd.DataFrame({
    'Feature': X_test.columns,
    'SHAP': shap_values[bad_client_idx],
    'Valeur': X_test.iloc[bad_client_idx].values
}).sort_values('SHAP', ascending=False).head(5)
for i, row in enumerate(top_positive.itertuples(), 1):
    print(f"{i}. {row.Feature:<30s} = {row.Valeur:>10.2f} | Impact: {row.SHAP:>8.4f}")
print("="*80)
```

**üìä Graphiques attendus** : 
- 2 Force plots (un par type de client)
- 2 Waterfall plots montrant la cascade des contributions

**Utilit√© Business** : 
Cette analyse permet d'expliquer √† un client **pourquoi** sa demande a √©t√© refus√©e ou accept√©e, avec des facteurs concrets et actionnables. Cela r√©pond aux exigences de **transparence** du RGPD et am√©liore la **relation client**.

**Exemple de feedback client** :
> "Votre demande a √©t√© refus√©e principalement en raison de : (1) 3 retards de paiement dans les 12 derniers mois, (2) un ratio dette/revenu de 52% (seuil : 40%), (3) un historique de cr√©dit court (2 ans). Pour am√©liorer vos chances, nous recommandons de r√©duire votre endettement et de consolider votre historique."

#### 8.4.4 Coefficients de R√©gression Logistique

```python
# Pour la r√©gression logistique (naturellement interpr√©table)
coef_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': logreg_best.coef_[0]
}).sort_values('Coefficient', key=abs, ascending=False)

print("Top 10 Features par impact (R√©gression Logistique):")
print(coef_df.head(10))

# Visualisation
plt.figure(figsize=(10, 8))
coef_df.head(15).plot(kind='barh', x='Feature', y='Coefficient')
plt.title('Coefficients de la R√©gression Logistique')
plt.xlabel('Coefficient (Log-Odds)')
plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)
plt.tight_layout()
```

**Interpr√©tation** :
- Coefficient positif ‚Üí augmente probabilit√© de d√©faut
- Coefficient n√©gatif ‚Üí diminue probabilit√© de d√©faut
- Magnitude indique la force de l'effet

---

## 9. Conclusion

### 9.1 Synth√®se des R√©sultats

**Mod√®le retenu** : [√Ä compl√©ter selon les r√©sultats - probablement XGBoost]

**Performances atteintes** :
- AUC-ROC : [X.XX] (objectif : > 0.80) ‚úì
- Recall : [X.XX] (objectif : > 0.70) ‚úì
- Precision : [X.XX] (objectif : > 0.60) ‚úì

**Facteurs pr√©dictifs cl√©s identifi√©s** :
1. Nombre de retards de paiement ant√©rieurs
2. Ratio dette/revenu
3. Revenu mensuel
4. Anciennet√© de l'historique de cr√©dit
5. Variables cr√©√©es (ratios financiers)

**Valeur Business** :
- R√©duction potentielle du taux de d√©faut de [X]%
- √âconomies estim√©es √† [X]‚Ç¨ par an
- Temps de traitement des demandes : de 3 jours ‚Üí instantan√©
- Am√©lioration de l'exp√©rience client

### 9.2 Limites du Mod√®le

#### 9.2.1 Limites Techniques

**D√©s√©quilibre des classes** : Malgr√© les techniques de r√©√©quilibrage (SMOTE, ajustement des poids), le mod√®le peut encore sous-performer sur la classe minoritaire.

**Features limit√©es** : L'absence de certaines donn√©es (ex : comportement de paiement mensuel, transactions r√©centes) limite la pr√©cision.

**Donn√©es statiques** : Le mod√®le ne capture pas les √©volutions temporelles (changement de situation professionnelle, √©v√©nements de vie).

**Overfitting potentiel** : Malgr√© la validation crois√©e, le mod√®le pourrait ne pas g√©n√©raliser parfaitement sur des donn√©es futures tr√®s diff√©rentes.

#### 9.2.2 Limites √âthiques et R√©glementaires

**Biais potentiels** : 
- Biais de s√©lection : donn√©es uniquement sur cr√©dits pass√©s
- Biais d√©mographiques : le mod√®le peut reproduire des discriminations historiques
- Analyse de fairness n√©cessaire (parit√© d√©mographique, √©galit√© des chances)

**Explicabilit√©** :
- Les mod√®les complexes (XGBoost) sont moins interpr√©tables
- N√©cessit√© d'outils compl√©mentaires (SHAP) pour la transparence
- Conformit√© RGPD : droit √† l'explication

**Zone grise juridique** :
- Utilisation de variables sensibles (√¢ge, genre) peut √™tre interdite dans certains pays
- N√©cessit√© d'un audit juridique avant d√©ploiement

#### 9.2.3 Limites Business

**Co√ªt des erreurs asym√©trique** : Un faux n√©gatif co√ªte 20x plus cher qu'un faux positif ‚Üí le seuil de d√©cision doit √™tre ajust√©.

**√âvolution du contexte √©conomique** : Une r√©cession ou crise peut rendre le mod√®le obsol√®te rapidement.

**Acceptabilit√© client** : Certains clients peuvent contester les d√©cisions automatis√©es.

### 9.3 Pistes d'Am√©lioration

#### 9.3.1 Am√©liorations Techniques

**Enrichissement des donn√©es** :
- Int√©grer des donn√©es externes : bureaux de cr√©dit, r√©seaux sociaux (avec consentement)
- Donn√©es temporelles : historique de transactions sur 12 mois
- Donn√©es alternatives : paiement loyer, factures utilities

**Techniques avanc√©es** :
- Stacking/Ensembling : combiner les pr√©dictions de plusieurs mod√®les
- Deep Learning : r√©seaux de neurones pour capturer des interactions complexes
- AutoML : automatiser la s√©lection et l'optimisation des mod√®les

**Calibration des probabilit√©s** :
- Utiliser Platt Scaling ou Isotonic Regression pour am√©liorer la calibration
- Les probabilit√©s pr√©dites refl√©teraient mieux les vraies probabilit√©s

**Gestion du d√©s√©quilibre** :
- Tester d'autres techniques : ADASYN, BorderlineSMOTE
- Apprentissage √† co√ªt sensible (cost-sensitive learning)
- Ajuster le seuil de d√©cision selon une analyse co√ªt-b√©n√©fice

#### 9.3.2 Am√©liorations du Feature Engineering

**Variables temporelles** :
- Tendances sur 3, 6, 12 mois (revenu, d√©penses)
- Saisonnalit√© des comportements financiers
- Ratio de croissance du revenu

**Agr√©gations avanc√©es** :
- Clustering de comportements (segments de clients)
- Scores composites pond√©r√©s par importance SHAP
- Interactions de 3√®me niveau

**Donn√©es textuelles** :
- NLP sur les motifs de demande de cr√©dit
- Analyse de sentiment des commentaires clients

#### 9.3.3 Validation et Monitoring

**Validation temporelle** :
- Train sur donn√©es 2020-2022, valider sur 2023, tester sur 2024
- V√©rifier la stabilit√© des performances dans le temps

**A/B Testing** :
- D√©ployer le mod√®le sur 10% des demandes
- Comparer performances vs processus manuel
- It√©rer selon feedback

**Monitoring continu** :
- Dashboard temps r√©el des performances (AUC, Recall, taux de d√©faut r√©el)
- Alertes si drift d√©tect√© (changement de distribution)
- R√©entra√Ænement trimestriel avec nouvelles donn√©es

#### 9.3.4 Aspects √âthiques et R√©glementaires

**Audit de fairness** :
- Mesurer les disparit√©s entre groupes d√©mographiques
- Appliquer des contraintes de parit√© si n√©cessaire
- Documentation transparente des biais identifi√©s

**Explicabilit√© renforc√©e** :
- Interface utilisateur montrant les facteurs de d√©cision
- Argumentaire clair pour chaque refus
- Possibilit√© de contestation humaine

**Gouvernance** :
- Comit√© d'√©thique pour superviser l'utilisation du mod√®le
- Audits r√©guliers par tiers ind√©pendants
- Conformit√© GDPR, B√¢le III, directives BCE

#### 9.3.5 D√©ploiement en Production

**Architecture** :
- API REST avec FastAPI ou Flask
- Conteneurisation avec Docker
- Orchestration avec Kubernetes pour la scalabilit√©

**MLOps** :
- Versioning des mod√®les avec MLflow ou DVC
- Pipeline CI/CD automatis√©
- Tests automatiques (unit tests, integration tests)

**Infrastructure** :
- Cloud (AWS SageMaker, Google Vertex AI, Azure ML)
- Syst√®me de cache pour pr√©dictions fr√©quentes
- Load balancing pour haute disponibilit√©

---

## 10. R√©f√©rences

### Articles Acad√©miques
1. Hand, D. J., & Henley, W. E. (1997). Statistical classification methods in consumer credit scoring: a review. *Journal of the Royal Statistical Society*, 160(3), 523-541.

2. Baesens, B., et al. (2003). Benchmarking state-of-the-art classification algorithms for credit scoring. *Journal of the Operational Research Society*, 54(6), 627-635.

3. Lessmann, S., et al. (2015). Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research. *European Journal of Operational Research*, 247(1), 124-136.

### Documentation Technique
4. Scikit-learn Documentation. https://scikit-learn.org/
5. XGBoost Documentation. https://xgboost.readthedocs.io/
6. SHAP Documentation. https://shap.readthedocs.io/

### Ressources en ligne
7. Kaggle - Credit Scoring Dataset. https://www.kaggle.com/datasets/kapturovalexander/bank-credit-scoring/
8. Towards Data Science - Credit Risk Modeling. https://towardsdatascience.com/

### R√©glementation
9. R√®glement G√©n√©ral sur la Protection des Donn√©es (RGPD). https://gdpr.eu/
10. Basel Committee on Banking Supervision. https://www.bis.org/bcbs/

---

## 11. Annexes

### Annexe A : Structure du Projet GitHub

```
bank-credit-scoring/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                      # Donn√©es brutes
‚îÇ   ‚îú‚îÄ‚îÄ processed/                # Donn√©es pr√©trait√©es
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Description des donn√©es
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA.ipynb             # Analyse exploratoire
‚îÇ   ‚îú‚îÄ‚îÄ 02_Preprocessing.ipynb    # Pr√©traitement
‚îÇ   ‚îú‚îÄ‚îÄ 03_Modeling.ipynb         # Mod√©lisation
‚îÇ   ‚îî‚îÄ‚îÄ 04_Evaluation.ipynb       # √âvaluation
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_processing.py        # Fonctions de preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py    # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ modeling.py               # Classes de mod√®les
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py             # M√©triques et √©valuation
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ logreg_best.pkl          # Mod√®le sauvegard√©
‚îÇ   ‚îú‚îÄ‚îÄ rf_best.pkl
‚îÇ   ‚îî‚îÄ‚îÄ xgb_best.pkl
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ figures/                  # Graphiques g√©n√©r√©s
‚îÇ   ‚îî‚îÄ‚îÄ rapport_final.pdf         # Ce rapport
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                     # Page d'accueil du projet
‚îú‚îÄ‚îÄ .gitignore                    # Fichiers √† ignorer
‚îî‚îÄ‚îÄ LICENSE                       # Licence du projet
```

### Annexe B : Commandes d'Installation

```bash
# Cloner le repository
git clone https://github.com/[username]/bank-credit-scoring.git
cd bank-credit-scoring

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer Jupyter
jupyter notebook
```

### Annexe C : Exemple de requirements.txt

```
pandas==2.1.0
numpy==1.24.0
matplotlib==3.7.1
seaborn==0.12.2
plotly==5.14.1
scikit-learn==1.3.0
xgboost==1.7.6
lightgbm==4.0.0
catboost==1.2
imbalanced-learn==0.11.0
shap==0.42.1
jupyter==1.0.0
notebook==7.0.0
```

### Annexe D : Glossaire

**AUC-ROC** : Area Under the Receiver Operating Characteristic curve. M√©trique mesurant la capacit√© du mod√®le √† distinguer les classes.

**Recall (Sensibilit√©)** : Proportion de vrais positifs correctement identifi√©s. Crucial pour minimiser les faux n√©gatifs.

**Precision** : Proportion de pr√©dictions positives qui sont correctes. Important pour √©viter les faux positifs.

**F1-Score** : Moyenne harmonique de Precision et Recall. √âquilibre entre les deux m√©triques.

**Cross-Validation** : Technique de validation en s√©parant les donn√©es en K parties pour entra√Æner et tester le mod√®le K fois.

**Overfitting** : Le mod√®le apprend trop bien les donn√©es d'entra√Ænement et ne g√©n√©ralise pas sur de nouvelles donn√©es.

**Feature Engineering** : Processus de cr√©ation de nouvelles variables √† partir des variables existantes.

**SMOTE** : Synthetic Minority Over-sampling Technique. M√©thode pour g√©rer le d√©s√©quilibre de classes.

**SHAP** : SHapley Additive exPlanations. Technique d'interpr√©tabilit√© bas√©e sur la th√©orie des jeux.

---

**Date de r√©daction** : D√©cembre 2025  
**Version** : 1.0  
**Contact** : [Votre email]  
**Repository GitHub** : [Lien vers votre repo]

---

*Ce rapport a √©t√© r√©dig√© dans le cadre du module Data Science & Machine Learning (2025-2026) sous la supervision du Professeur A. Larhlimi.*
