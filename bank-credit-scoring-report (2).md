# Projet Data Science : Credit Scoring Bancaire

## Informations du Projet

**Auteur** : [Votre Nom]  
**Module** : Data Science & Machine Learning  
**Ann√©e Universitaire** : 2025-2026  
**Enseignant** : A. Larhlimi  
**Th√©matique** : Finance - Credit Scoring

---

## üìã Sommaire

1. [Introduction](#1-introduction)
   - 1.1 [Contexte de la Mission](#11-contexte-de-la-mission)
   - 1.2 [Probl√©matique](#12-probl√©matique)
   - 1.3 [Objectifs du Projet](#13-objectifs-du-projet)
2. [Th√©matique : Credit Scoring](#2-th√©matique--credit-scoring)
   - 2.1 [D√©finition du Credit Scoring](#21-d√©finition-du-credit-scoring)
   - 2.2 [Enjeux Business](#22-enjeux-business)
   - 2.3 [Type de Machine Learning](#23-type-de-machine-learning)
3. [Pr√©sentation du Dataset](#3-pr√©sentation-du-dataset)
   - 3.1 [Source des Donn√©es](#31-source-des-donn√©es)
   - 3.2 [Description G√©n√©rale](#32-description-g√©n√©rale)
   - 3.3 [Dictionnaire des Variables](#33-dictionnaire-des-variables)
   - 3.4 [Variable Cible](#34-variable-cible)
4. [M√©thodologie](#4-m√©thodologie)
   - 4.1 [Pipeline de Travail](#41-pipeline-de-travail)
   - 4.2 [Outils et Technologies](#42-outils-et-technologies)
5. [Pr√©traitement des Donn√©es](#5-pr√©traitement-des-donn√©es)
   - 5.1 [Nettoyage des Donn√©es](#51-nettoyage-des-donn√©es)
   - 5.2 [Gestion des Valeurs Manquantes](#52-gestion-des-valeurs-manquantes)
   - 5.3 [Encodage des Variables Cat√©gorielles](#53-encodage-des-variables-cat√©gorielles)
   - 5.4 [Normalisation et Standardisation](#54-normalisation-et-standardisation)
6. [Analyse Exploratoire des Donn√©es (EDA)](#6-analyse-exploratoire-des-donn√©es-eda)
   - 6.1 [Statistiques Descriptives](#61-statistiques-descriptives)
   - 6.2 [Visualisation des Distributions](#62-visualisation-des-distributions)
   - 6.3 [Analyse des Corr√©lations](#63-analyse-des-corr√©lations)
   - 6.4 [Feature Engineering](#64-feature-engineering)
7. [Mod√©lisation Machine Learning](#7-mod√©lisation-machine-learning)
   - 7.1 [S√©paration Train/Test](#71-s√©paration-traintest)
   - 7.2 [S√©lection des Algorithmes](#72-s√©lection-des-algorithmes)
   - 7.3 [Validation Crois√©e](#73-validation-crois√©e)
   - 7.4 [Optimisation des Hyperparam√®tres](#74-optimisation-des-hyperparam√®tres)
8. [R√©sultats et Discussion](#8-r√©sultats-et-discussion)
   - 8.1 [M√©triques de Performance](#81-m√©triques-de-performance)
   - 8.2 [Comparaison des Mod√®les](#82-comparaison-des-mod√®les)
   - 8.3 [Analyse des Erreurs](#83-analyse-des-erreurs)
   - 8.4 [Interpr√©tabilit√© du Mod√®le](#84-interpr√©tabilit√©-du-mod√®le)
9. [Conclusion](#9-conclusion)
   - 9.1 [Synth√®se des R√©sultats](#91-synth√®se-des-r√©sultats)
   - 9.2 [Limites du Mod√®le](#92-limites-du-mod√®le)
   - 9.3 [Pistes d'Am√©lioration](#93-pistes-dam√©lioration)
10. [R√©f√©rences](#10-r√©f√©rences)
11. [Annexes](#11-annexes)

---

## 1. Introduction

### 1.1 Contexte de la Mission

Dans le cadre du module Data Science & Machine Learning de l'ann√©e universitaire 2025-2026, ce projet nous place dans la position d'un Data Scientist au sein d'un cabinet d'√©tudes strat√©giques sp√©cialis√© dans le secteur financier. La mission consiste √† d√©velopper un syst√®me de credit scoring permettant d'√©valuer automatiquement la solvabilit√© des clients demandeurs de cr√©dit.

Le secteur bancaire fait face √† un d√©fi majeur : accorder des cr√©dits tout en minimisant le risque de d√©faut de paiement. Chaque ann√©e, les pertes li√©es aux cr√©dits non rembours√©s repr√©sentent des milliards d'euros pour les institutions financi√®res. Dans ce contexte, les techniques de Machine Learning offrent des opportunit√©s consid√©rables pour am√©liorer la pr√©cision des d√©cisions d'octroi de cr√©dit.

### 1.2 Probl√©matique

**Question centrale** : Comment pr√©dire avec pr√©cision si un client sera en d√©faut de paiement ou non, sur la base de ses caract√©ristiques d√©mographiques, financi√®res et comportementales ?

Cette probl√©matique soul√®ve plusieurs sous-questions :
- Quelles sont les variables les plus pr√©dictives du risque de d√©faut ?
- Comment traiter le d√©s√©quilibre potentiel entre bons et mauvais payeurs ?
- Quel mod√®le offre le meilleur compromis entre performance et interpr√©tabilit√© ?
- Comment minimiser les erreurs co√ªteuses (faux n√©gatifs) tout en √©vitant de rejeter des clients solvables (faux positifs) ?

### 1.3 Objectifs du Projet

**Objectif principal** : D√©velopper un mod√®le de classification binaire capable de pr√©dire le risque de d√©faut de paiement avec une pr√©cision sup√©rieure √† 80% (AUC-ROC).

**Objectifs secondaires** :
1. **Exploration** : Comprendre les patterns et relations dans les donn√©es de cr√©dit
2. **Transformation** : Nettoyer et pr√©parer les donn√©es pour la mod√©lisation
3. **Mod√©lisation** : Comparer au moins trois algorithmes de Machine Learning diff√©rents
4. **Optimisation** : Affiner les hyperparam√®tres pour maximiser les performances
5. **Interpr√©tation** : Identifier les facteurs cl√©s influen√ßant le risque de cr√©dit
6. **Communication** : Pr√©senter les r√©sultats de mani√®re claire et exploitable

---

## 2. Th√©matique : Credit Scoring

### 2.1 D√©finition du Credit Scoring

Le credit scoring est une m√©thode statistique permettant d'√©valuer la probabilit√© qu'un emprunteur rembourse son cr√©dit. Il s'agit d'attribuer un score num√©rique √† chaque demandeur, refl√©tant son niveau de risque. Plus le score est √©lev√©, plus le client est consid√©r√© comme fiable.

Cette technique est utilis√©e pour :
- L'octroi de pr√™ts personnels et immobiliers
- L'attribution de cartes de cr√©dit
- Le calcul des taux d'int√©r√™t personnalis√©s
- La gestion du portefeuille de cr√©dit

### 2.2 Enjeux Business

#### Pour la Banque
- **R√©duction des pertes** : Diminution du taux de d√©faut de paiement
- **Optimisation du capital** : Allocation efficace des ressources financi√®res
- **Automatisation** : Acc√©l√©ration du processus de d√©cision (de plusieurs jours √† quelques minutes)
- **Conformit√© r√©glementaire** : Respect des normes B√¢le II/III

#### Pour les Clients
- **Rapidit√©** : R√©ponse instantan√©e sur l'√©ligibilit√© au cr√©dit
- **√âquit√©** : D√©cisions bas√©es sur des crit√®res objectifs et mesurables
- **Personnalisation** : Offres adapt√©es au profil de risque

#### Impact √âconomique
Le march√© mondial du cr√©dit repr√©sente plusieurs trillions d'euros. Une am√©lioration de 1% dans la pr√©diction du risque peut g√©n√©rer des √©conomies de plusieurs millions d'euros pour une institution financi√®re de taille moyenne.

### 2.3 Type de Machine Learning

**Classification Binaire Supervis√©e**

- **Type** : Apprentissage supervis√© (Supervised Learning)
- **Cat√©gorie** : Classification binaire
- **Classes** : 
  - Classe 0 : Bon payeur (pas de d√©faut)
  - Classe 1 : Mauvais payeur (d√©faut de paiement)
- **Input** : Features (variables explicatives) sur le profil client
- **Output** : Probabilit√© de d√©faut et classe pr√©dite (0 ou 1)

**Justification du choix** : La nature binaire du probl√®me (d√©faut/pas de d√©faut) et la disponibilit√© de donn√©es historiques √©tiquet√©es en font un cas typique de classification supervis√©e.

---

## 3. Pr√©sentation du Dataset

### 3.1 Source des Donn√©es

**Origine** : Kaggle - "Credit Scoring for Borrowers in Bank"  
**Auteur** : kapturovalexander  
**URL** : https://www.kaggle.com/datasets/kapturovalexander/bank-credit-scoring/data  
**Licence** : [√Ä pr√©ciser selon Kaggle]  
**Date de collecte** : [√Ä pr√©ciser]

**Justification du choix** : Ce dataset a √©t√© s√©lectionn√© pour plusieurs raisons :
- Richesse des variables (d√©mographiques, financi√®res, comportementales)
- Taille suffisante pour l'entra√Ænement de mod√®les robustes
- Probl√©matique r√©aliste et applicable en contexte professionnel
- Complexit√© adapt√©e au niveau du module

### 3.2 Description G√©n√©rale

**M√©tadonn√©es du Dataset** :

| Caract√©ristique | Valeur |
|-----------------|--------|
| Nombre d'observations | [√Ä compl√©ter apr√®s chargement] |
| Nombre de variables | [√Ä compl√©ter apr√®s chargement] |
| Variables num√©riques | [√Ä compl√©ter] |
| Variables cat√©gorielles | [√Ä compl√©ter] |
| Variable cible | [Nom de la variable] |
| P√©riode couverte | [Si applicable] |
| Taux de valeurs manquantes | [√Ä calculer] |
| D√©s√©quilibre des classes | [Ratio bon/mauvais payeurs] |

### 3.3 Dictionnaire des Variables

Le dataset contient g√©n√©ralement les types de variables suivants (√† adapter selon le dataset r√©el) :

#### Variables D√©mographiques

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `age` | Num√©rique | √Çge du client en ann√©es | 25, 45, 62 |
| `gender` | Cat√©gorielle | Genre du client | M, F |
| `marital_status` | Cat√©gorielle | Statut marital | Single, Married, Divorced |
| `dependents` | Num√©rique | Nombre de personnes √† charge | 0, 1, 2, 3+ |
| `education` | Cat√©gorielle | Niveau d'√©ducation | High School, Bachelor, Master, PhD |

#### Variables Professionnelles

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `employment_type` | Cat√©gorielle | Type d'emploi | Full-time, Part-time, Self-employed |
| `job_tenure` | Num√©rique | Anciennet√© dans l'emploi (ann√©es) | 1, 5, 10 |
| `income` | Num√©rique | Revenu mensuel/annuel (‚Ç¨) | 2000, 3500, 5000 |
| `industry` | Cat√©gorielle | Secteur d'activit√© | IT, Finance, Healthcare |

#### Variables Financi√®res

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `loan_amount` | Num√©rique | Montant du pr√™t demand√© (‚Ç¨) | 5000, 15000, 30000 |
| `loan_term` | Num√©rique | Dur√©e du pr√™t (mois) | 12, 24, 36, 60 |
| `interest_rate` | Num√©rique | Taux d'int√©r√™t (%) | 3.5, 5.2, 7.8 |
| `debt_to_income` | Num√©rique | Ratio dette/revenu | 0.2, 0.35, 0.5 |
| `credit_history_length` | Num√©rique | Anciennet√© historique cr√©dit (ann√©es) | 3, 7, 15 |

#### Variables Comportementales

| Variable | Type | Description | Exemple de valeurs |
|----------|------|-------------|-------------------|
| `num_credit_lines` | Num√©rique | Nombre de lignes de cr√©dit | 1, 3, 5 |
| `num_late_payments` | Num√©rique | Nombre de retards de paiement | 0, 1, 2 |
| `has_mortgage` | Binaire | Poss√®de un pr√™t immobilier | 0 (Non), 1 (Oui) |
| `has_car_loan` | Binaire | Poss√®de un pr√™t auto | 0 (Non), 1 (Oui) |
| `credit_utilization` | Num√©rique | Taux d'utilisation du cr√©dit (%) | 15, 45, 80 |

### 3.4 Variable Cible

**Nom** : `default` (ou √©quivalent selon le dataset)  
**Type** : Binaire (0/1)  
**Signification** :
- **0** : Client sans d√©faut de paiement (bon payeur)
- **1** : Client en d√©faut de paiement (mauvais payeur)

**D√©finition du d√©faut** : Un d√©faut de paiement est g√©n√©ralement d√©fini comme un retard de paiement sup√©rieur √† 90 jours cons√©cutifs.

---

## 4. M√©thodologie

### 4.1 Pipeline de Travail

Notre approche suit le cycle de vie standard d'un projet de Machine Learning :

```
1. Compr√©hension du probl√®me business
   ‚Üì
2. Collecte et exploration des donn√©es (EDA)
   ‚Üì
3. Nettoyage et pr√©traitement
   ‚Üì
4. Feature Engineering
   ‚Üì
5. S√©paration des donn√©es (Train/Validation/Test)
   ‚Üì
6. Entra√Ænement de mod√®les multiples
   ‚Üì
7. Validation crois√©e
   ‚Üì
8. Optimisation des hyperparam√®tres
   ‚Üì
9. √âvaluation et comparaison des mod√®les
   ‚Üì
10. S√©lection du meilleur mod√®le
   ‚Üì
11. √âvaluation finale sur le jeu de test
   ‚Üì
12. Interpr√©tation et analyse
   ‚Üì
13. Documentation et pr√©sentation
```

### 4.2 Outils et Technologies

**Langage** : Python 3.10+

**Biblioth√®ques principales** :

```python
# Manipulation de donn√©es
pandas==2.1.0
numpy==1.24.0

# Visualisation
matplotlib==3.7.1
seaborn==0.12.2
plotly==5.14.1

# Preprocessing
scikit-learn==1.3.0
imbalanced-learn==0.11.0

# Mod√©lisation
xgboost==1.7.6
lightgbm==4.0.0
catboost==1.2

# √âvaluation et interpr√©tabilit√©
shap==0.42.1
lime==0.2.0.1

# Utilitaires
jupyter==1.0.0
tqdm==4.65.0
```

**Environnement de d√©veloppement** :
- Jupyter Notebook pour l'exploration interactive
- Git/GitHub pour le versioning
- Visual Studio Code pour l'√©dition de code

---

## 5. Pr√©traitement des Donn√©es

### 5.1 Nettoyage des Donn√©es

**Objectif** : Garantir la qualit√© et la coh√©rence des donn√©es avant toute analyse.

#### 5.1.1 D√©tection et Suppression des Doublons

**Strat√©gie** :
```python
# Identification des doublons
duplicates = df.duplicated().sum()
print(f"Nombre de lignes dupliqu√©es : {duplicates}")

# Suppression des doublons
df_clean = df.drop_duplicates()
```

**Justification** : Les doublons peuvent biaiser les statistiques et la performance du mod√®le. Ils sont supprim√©s sauf si justifi√©s business (ex : un client ayant plusieurs pr√™ts).

#### 5.1.2 Formatage des Donn√©es

**Actions r√©alis√©es** :
- Conversion des types de donn√©es (ex : strings ‚Üí numeric)
- Standardisation des formats de dates
- Correction des valeurs aberrantes √©videntes (ex : √¢ge n√©gatif)
- Uniformisation des cat√©gories (ex : "Male"/"M" ‚Üí "M")

#### 5.1.3 D√©tection des Outliers

**M√©thode IQR (Interquartile Range)** :
```python
Q1 = df['income'].quantile(0.25)
Q3 = df['income'].quantile(0.75)
IQR = Q3 - Q1
outliers = (df['income'] < Q1 - 1.5*IQR) | (df['income'] > Q3 + 1.5*IQR)
```

**Traitement** : Les outliers sont analys√©s au cas par cas. Certains sont l√©gitimes (ex : tr√®s hauts revenus) et conserv√©s, d'autres sont plafonn√©s (capping) ou supprim√©s.

### 5.2 Gestion des Valeurs Manquantes

**Analyse pr√©alable** :
```python
missing_percent = (df.isnull().sum() / len(df)) * 100
print(missing_percent[missing_percent > 0].sort_values(ascending=False))
```

#### 5.2.1 Strat√©gies d'Imputation

**Pour les variables num√©riques** :

| Variable | Taux de manquants | Strat√©gie | Justification |
|----------|-------------------|-----------|---------------|
| `income` | < 5% | M√©diane | Robuste aux outliers |
| `credit_history_length` | < 10% | Moyenne | Distribution normale |
| `debt_to_income` | > 20% | KNN Imputer | Pr√©serve les relations |

**Pour les variables cat√©gorielles** :

| Variable | Taux de manquants | Strat√©gie | Justification |
|----------|-------------------|-----------|---------------|
| `education` | < 5% | Mode | Valeur la plus fr√©quente |
| `employment_type` | > 10% | Cat√©gorie "Unknown" | Informative en soi |

#### 5.2.2 Imputation Avanc√©e

**KNN Imputer** : Pour les variables avec patterns complexes
```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df_imputed = imputer.fit_transform(df_numeric)
```

**Justification** : KNN pr√©serve les relations entre variables en utilisant les K plus proches voisins pour estimer les valeurs manquantes.

### 5.3 Encodage des Variables Cat√©gorielles

#### 5.3.1 Label Encoding

**Pour les variables ordinales** :
```python
from sklearn.preprocessing import LabelEncoder

# Education : ordinalit√© claire
education_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}
df['education_encoded'] = df['education'].map(education_order)
```

**Justification** : L'ordre est significatif et doit √™tre pr√©serv√©.

#### 5.3.2 One-Hot Encoding

**Pour les variables nominales** :
```python
# Variables sans ordre intrins√®que
df_encoded = pd.get_dummies(df, columns=['industry', 'marital_status'], 
                             drop_first=True)
```

**Justification** : √âvite d'introduire un ordre artificiel. Le param√®tre `drop_first=True` √©vite la multicolin√©arit√©.

#### 5.3.3 Target Encoding

**Pour les variables √† haute cardinalit√©** :
```python
# Si 'job_title' a 100+ cat√©gories
target_mean = df.groupby('job_title')['default'].mean()
df['job_title_encoded'] = df['job_title'].map(target_mean)
```

**Justification** : R√©duit la dimensionnalit√© tout en capturant l'information relative √† la cible.

### 5.4 Normalisation et Standardisation

#### 5.4.1 Standardisation (Z-score)

**Formule** : `z = (x - Œº) / œÉ`

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features = ['income', 'loan_amount', 'age']
df[numerical_features] = scaler.fit_transform(df[numerical_features])
```

**Justification** : N√©cessaire pour les algorithmes sensibles √† l'√©chelle (SVM, R√©gression Logistique, KNN).

#### 5.4.2 Normalisation Min-Max

**Formule** : `x_norm = (x - x_min) / (x_max - x_min)`

**Utilisation** : Pour les features devant rester dans [0, 1], notamment pour les r√©seaux de neurones.

**Choix** : Nous privil√©gions la standardisation pour ce projet car elle est plus robuste aux outliers.

---

## 6. Analyse Exploratoire des Donn√©es (EDA)

### 6.1 Statistiques Descriptives

**R√©sum√© des variables num√©riques** :
```python
df.describe().T
```

**Interpr√©tation attendue** :
- **√Çge moyen** : ~40 ans (population active)
- **Revenu m√©dian** : ~2500-3000‚Ç¨
- **Montant moyen de pr√™t** : ~15000‚Ç¨
- **Taux de d√©faut** : ~10-20% (d√©s√©quilibre typique)

### 6.2 Visualisation des Distributions

#### 6.2.1 Variables Num√©riques

**Histogrammes** :
```python
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
numerical_cols = ['age', 'income', 'loan_amount', 'debt_to_income', 
                  'credit_history_length', 'num_credit_lines']

for i, col in enumerate(numerical_cols):
    ax = axes[i//3, i%3]
    df[col].hist(bins=30, ax=ax, edgecolor='black')
    ax.set_title(f'Distribution de {col}')
    ax.set_xlabel(col)
    ax.set_ylabel('Fr√©quence')
```

**Interpr√©tation** :
- **Revenu** : Distribution asym√©trique (long tail √† droite) ‚Üí n√©cessite transformation log
- **√Çge** : Distribution relativement normale avec pic 30-50 ans
- **Montant du pr√™t** : Distribution multimodale ‚Üí segments de clients diff√©rents

#### 6.2.2 Boxplots pour Outliers

```python
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[numerical_cols])
plt.xticks(rotation=45)
plt.title('D√©tection des Outliers')
```

**Interpr√©tation** : Les outliers dans `income` et `loan_amount` sont conserv√©s car l√©gitimes.

#### 6.2.3 Variables Cat√©gorielles

```python
categorical_cols = ['gender', 'education', 'employment_type', 'marital_status']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
for i, col in enumerate(categorical_cols):
    ax = axes[i//2, i%2]
    df[col].value_counts().plot(kind='bar', ax=ax)
    ax.set_title(f'Distribution de {col}')
    ax.set_ylabel('Count')
```

**Interpr√©tation** :
- Majorit√© de clients mari√©s et employ√©s √† temps plein
- Distribution √©quilibr√©e entre genres
- Niveau Bachelor majoritaire

### 6.3 Analyse des Corr√©lations

#### 6.3.1 Heatmap de Corr√©lation

```python
plt.figure(figsize=(14, 10))
correlation_matrix = df[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', 
            center=0, fmt='.2f')
plt.title('Matrice de Corr√©lation des Variables Num√©riques')
```

**Interpr√©tation attendue** :
- **Corr√©lation positive forte** : `loan_amount` et `income` (0.6-0.7)
- **Corr√©lation mod√©r√©e** : `age` et `credit_history_length` (0.5)
- **Corr√©lation n√©gative** : `num_late_payments` et `credit_score` (-0.4)

#### 6.3.2 Corr√©lation avec la Cible

```python
target_corr = df.corr()['default'].sort_values(ascending=False)
print(target_corr)

# Visualisation
plt.figure(figsize=(10, 6))
target_corr.plot(kind='barh')
plt.title('Corr√©lation des Features avec le D√©faut de Paiement')
```

**Variables pr√©dictives attendues** :
- `num_late_payments` : forte corr√©lation positive
- `debt_to_income` : corr√©lation positive
- `income` : corr√©lation n√©gative
- `credit_history_length` : corr√©lation n√©gative

### 6.4 Feature Engineering

**Cr√©ation de nouvelles variables pertinentes** :

#### 6.4.1 Ratios Financiers

```python
# Ratio mensualit√©/revenu
df['payment_to_income'] = (df['loan_amount'] / df['loan_term']) / df['income']

# Capacit√© d'√©pargne
df['savings_capacity'] = df['income'] - (df['income'] * df['debt_to_income'])

# Ratio cr√©dit utilis√©
df['credit_usage_ratio'] = df['num_credit_lines'] / df['credit_history_length']
```

**Justification** : Ces ratios capturent mieux la capacit√© de remboursement qu'une variable isol√©e.

#### 6.4.2 Variables Binaires

```python
# Client senior (> 60 ans)
df['is_senior'] = (df['age'] > 60).astype(int)

# Haut revenu (top 25%)
df['high_income'] = (df['income'] > df['income'].quantile(0.75)).astype(int)

# Historique cr√©dit long
df['long_credit_history'] = (df['credit_history_length'] > 10).astype(int)
```

**Justification** : Capture des seuils non-lin√©aires importants pour la d√©cision.

#### 6.4.3 Variables d'Interaction

```python
# Interaction √¢ge √ó revenu
df['age_income'] = df['age'] * df['income']

# Interaction √©ducation √ó emploi
df['edu_emp'] = df['education_encoded'] * df['employment_type_encoded']
```

**Justification** : Capture les effets combin√©s de plusieurs variables.

#### 6.4.4 Variables Agr√©g√©es

```python
# Score de risque composite
df['risk_score'] = (
    df['num_late_payments'] * 2 + 
    df['debt_to_income'] * 10 +
    (1 / (df['income'] + 1)) * 1000
)
```

**Justification** : Combine plusieurs indicateurs de risque en une m√©trique unique.

---

## 7. Mod√©lisation Machine Learning

### 7.1 S√©paration Train/Test

**Strat√©gie** : Split 80/20 avec stratification pour pr√©server le ratio de classes.

```python
from sklearn.model_selection import train_test_split

X = df.drop('default', axis=1)
y = df['default']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Train set: {X_train.shape}")
print(f"Test set: {X_test.shape}")
print(f"Distribution train: {y_train.value_counts(normalize=True)}")
print(f"Distribution test: {y_test.value_counts(normalize=True)}")
```

**Justification** :
- 80/20 offre suffisamment de donn√©es d'entra√Ænement tout en conservant un test robuste
- Stratification garantit la m√™me proportion de classes dans train et test
- `random_state=42` assure la reproductibilit√©

### 7.2 S√©lection des Algorithmes

Nous comparons trois familles d'algorithmes aux caract√©ristiques compl√©mentaires :

#### 7.2.1 R√©gression Logistique

**Caract√©ristiques** :
- Algorithme lin√©aire simple et interpr√©table
- Rapide √† entra√Æner
- Baseline de r√©f√©rence

**Impl√©mentation** :
```python
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(random_state=42, max_iter=1000)
logreg.fit(X_train, y_train)
```

**Avantages** : Coefficients interpr√©tables, probabilit√©s calibr√©es  
**Inconv√©nients** : Suppose une relation lin√©aire

#### 7.2.2 Random Forest

**Caract√©ristiques** :
- Ensemble d'arbres de d√©cision
- G√®re bien les non-lin√©arit√©s
- Robuste aux outliers

**Impl√©mentation** :
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
```

**Avantages** : Peu de preprocessing requis, importance des features  
**Inconv√©nients** : Peut overfitter, moins interpr√©table

#### 7.2.3 XGBoost (Gradient Boosting)

**Caract√©ristiques** :
- √âtat de l'art pour donn√©es tabulaires
- Boosting it√©ratif
- G√®re nativement les valeurs manquantes

**Impl√©mentation** :
```python
import xgboost as xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    eval_metric='logloss'
)
xgb_model.fit(X_train, y_train)
```

**Avantages** : Performances exceptionnelles, r√©gularisation int√©gr√©e  
**Inconv√©nients** : Temps de calcul plus long, n√©cessite tuning

**Justification du choix** : Ces trois algorithmes offrent une comparaison compl√®te entre approche lin√©aire, bagging et boosting.

### 7.3 Validation Crois√©e

**Strat√©gie** : K-Fold Cross-Validation stratifi√©e avec k=5

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Pour chaque mod√®le
for name, model in [('LogReg', logreg), ('RF', rf), ('XGB', xgb_model)]:
    cv_scores = cross_val_score(
        model, X_train, y_train, 
        cv=skf, 
        scoring='roc_auc',
        n_jobs=-1
    )
    print(f"{name} - CV AUC-ROC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
```

**Justification** :
- K=5 offre un bon compromis entre variance et biais
- Stratification maintient la distribution des classes dans chaque fold
- AUC-ROC est la m√©trique principale pour le d√©s√©quilibre de classes

### 7.4 Optimisation des Hyperparam√®tres

#### 7.4.1 Grid Search pour Random Forest

```python
from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

grid_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_rf,
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)
grid_rf.fit(X_train, y_train)

print(f"Meilleurs param√®tres RF: {grid_rf.best_params_}")
print(f"Meilleur score CV: {grid_rf.best_score_:.4f}")
```

#### 7.4.2 Randomized Search pour XGBoost

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

param_dist_xgb = {
    'n_estimators': randint(100, 500),
    'max_depth': randint(3, 10),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}

random_xgb = RandomizedSearchCV(
    xgb.XGBClassifier(random_state=42),
    param_dist_xgb,
    n_iter=50,
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1,
    random_state=42
)
random_xgb.fit(X_train, y_train)

print(f"Meilleurs param√®tres XGB: {random_xgb.best_params_}")
print(f"Meilleur score CV: {random_xgb.best_score_:.4f}")
```

**Justification** :
- GridSearch pour RF : espace de recherche raisonnable
- RandomizedSearch pour XGB : espace de recherche vaste, plus efficace
- N_iter=50 offre une bonne exploration sans temps excessif

---

## 8. R√©sultats et Discussion

### 8.1 M√©triques de Performance

**√âvaluation sur le jeu de test** :

```python
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                              f1_score, roc_auc_score, classification_report,
                              confusion_matrix, roc_curve)

def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'AUC-ROC': roc_auc_score(y_test, y_pred_proba)
    }
    
    print(f"\n=== {model_name} ===")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")
    
    return metrics, y_pred, y_pred_proba
```

**Tableau r√©capitulatif des performances** :

| Mod√®le | Accuracy | Precision | Recall | F1-Score | AUC-ROC |
|--------|----------|-----------|--------|----------|---------|
| R√©gression Logistique | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] |
| Random Forest | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] |
| XGBoost | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] | [√Ä remplir] |

### 8.2 Comparaison des Mod√®les

#### 8.2.1 Courbes ROC

```python
plt.figure(figsize=(10, 8))

for name, model in [('LogReg', logreg_best), ('RF', rf_best), ('XGB', xgb_best)]:
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Courbes ROC - Comparaison des Mod√®les')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

**Interpr√©tation attendue** :
- XGBoost devrait montrer la meilleure courbe (plus proche du coin sup√©rieur gauche)
- La diff√©rence entre les mod√®les indique le gain de complexit√©
- AUC > 0.80 est consid√©r√© comme bon pour le credit scoring

#### 8.2.2 Visualisation Comparative

```python
# Barplot comparatif des m√©triques
metrics_df = pd.DataFrame({
    'Logistic Regression': metrics_logreg,
    'Random Forest': metrics_rf,
    'XGBoost': metrics_xgb
})

metrics_df.T.plot(kind='bar', figsize=(12, 6))
plt.title('Comparaison des M√©triques entre Mod√®les')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
```

### 8.3 Analyse des Erreurs

#### 8.3.1 Matrice de Confusion

```python
from sklearn.metrics import ConfusionMatrixDisplay

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, (name, model) in enumerate([('LogReg', logreg_best), 
                                      ('RF', rf_best), 
                                      ('XGB', xgb_best)]):
    y_pred = model.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, 
                                   display_labels=['Bon Payeur', 'D√©faut'])
    disp.plot(ax=axes[idx], cmap='Blues')
    axes[idx].set_title(f'Matrice de Confusion - {name}')

plt.tight_layout()
```

**Analyse des erreurs** :

| Type d'erreur | Impact Business | Priorit√© |
|---------------|-----------------|----------|
| **Faux N√©gatifs** (FN) | Accorder cr√©dit √† mauvais payeur | **CRITIQUE** |
| **Faux Positifs** (FP) | Refuser cr√©dit √† bon payeur | Mod√©r√© |
| **Vrais Positifs** (TP) | Identifier correctement le risque | Positif |
| **Vrais N√©gatifs** (TN) | Identifier correctement la solvabilit√© | Positif |

**Calcul du co√ªt** :
```python
# Hypoth√®se de co√ªts
cost_FN = 10000  # Perte moyenne par d√©faut
cost_FP = 500     # Manque √† gagner par refus erron√©

total_cost = (FN * cost_FN) + (FP * cost_FP)
print(f"Co√ªt total des erreurs: {total_cost:,.0f}‚Ç¨")
```

#### 8.3.2 Analyse des Cas Mal Class√©s

```python
# Identifier les faux n√©gatifs (les plus co√ªteux)
false_negatives = X_test[(y_test == 1) & (y_pred == 0)]

print(f"Nombre de faux n√©gatifs: {len(false_negatives)}")
print("\nCaract√©ristiques moyennes des faux n√©gatifs:")
print(false_negatives.describe())

# Comparer avec les vrais positifs
true_positives = X_test[(y_test == 1) & (y_pred == 1)]
comparison = pd.DataFrame({
    'Faux N√©gatifs': false_negatives.mean(),
    'Vrais Positifs': true_positives.mean()
})
print("\nComparaison:")
print(comparison)
```

**Insights attendus** :
- Les faux n√©gatifs ont souvent des caract√©ristiques "borderline"
- Variables discriminantes insuffisamment captur√©es
- N√©cessit√© de features engineering additionnel ou ajustement du seuil

### 8.4 Interpr√©tabilit√© du Mod√®le

#### 8.4.1 Importance des Features (Random Forest & XGBoost)

```python
# Pour Random Forest
feature_importance_rf = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_best.feature_importances_
}).sort_values('Importance', ascending=False)

# Pour XGBoost
feature_importance_xgb = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': xgb_best.feature_importances_
}).sort_values('Importance', ascending=False)

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

feature_importance_rf.head(10).plot(kind='barh', x='Feature', y='Importance', 
                                     ax=axes[0], color='steelblue')
axes[0].set_title('Top 10 Features - Random Forest')
axes[0].invert_yaxis()

feature_importance_xgb.head(10).plot(kind='barh', x='Feature', y='Importance', 
                                      ax=axes[1], color='coral')
axes[1].set_title('Top 10 Features - XGBoost')
axes[1].invert_yaxis()

plt.tight_layout()
```

**Interpr√©tation attendue** :
- Les features cr√©√©es (ratios) devraient figurer en top positions
- `num_late_payments`, `debt_to_income`, `income` parmi les plus importantes
- Convergence entre RF et XGB confirme la robustesse

#### 8.4.2 SHAP Values (SHapley Additive exPlanations)

```python
import shap

# Cr√©er l'explainer pour XGBoost
explainer = shap.TreeExplainer(xgb_best)
shap_values = explainer.shap_values(X_test)

# Summary plot : Vue globale de l'impact des features
shap.summary_plot(shap_values, X_test, plot_type="bar")
plt.title('Impact Moyen des Features (SHAP)')

# Detailed summary plot
shap.summary_plot(shap_values, X_test)
plt.title('Distribution des SHAP Values')
```

**Interpr√©tation** :
- **Rouge** : valeurs √©lev√©es de la feature
- **Bleu** : valeurs faibles de la feature
- **Position horizontale** : impact sur la pr√©diction

**Exemple d'analyse** : Un `debt_to_income` √©lev√© (rouge) augmente la probabilit√© de d√©faut (d√©place vers la droite).

#### 8.4.3 Explication d'une Pr√©diction Individuelle

```python
# S√©lectionner un client
client_idx = 0
client_data = X_test.iloc[[client_idx]]

# SHAP Force Plot
shap.force_plot(explainer.expected_value, 
                shap_values[client_idx], 
                client_data,
                matplotlib=True)

# Waterfall plot
shap.waterfall_plot(shap.Explanation(values=shap_values[client_idx], 
                                     base_values=explainer.expected_value, 
                                     data=client_data.values[0],
                                     feature_names=X_test.columns.tolist()))
```

**Utilit√©** : Permet d'expliquer √† un client pourquoi sa demande a √©t√© refus√©e (transparence r√©glementaire).

#### 8.4.4 Coefficients de R√©gression Logistique

```python
# Pour la r√©gression logistique (naturellement interpr√©table)
coef_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': logreg_best.coef_[0]
}).sort_values('Coefficient', key=abs, ascending=False)

print("Top 10 Features par impact (R√©gression Logistique):")
print(coef_df.head(10))

# Visualisation
plt.figure(figsize=(10, 8))
coef_df.head(15).plot(kind='barh', x='Feature', y='Coefficient')
plt.title('Coefficients de la R√©gression Logistique')
plt.xlabel('Coefficient (Log-Odds)')
plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)
plt.tight_layout()
```

**Interpr√©tation** :
- Coefficient positif ‚Üí augmente probabilit√© de d√©faut
- Coefficient n√©gatif ‚Üí diminue probabilit√© de d√©faut
- Magnitude indique la force de l'effet

---

## 9. Conclusion

### 9.1 Synth√®se des R√©sultats

**Mod√®le retenu** : [√Ä compl√©ter selon les r√©sultats - probablement XGBoost]

**Performances atteintes** :
- AUC-ROC : [X.XX] (objectif : > 0.80) ‚úì
- Recall : [X.XX] (objectif : > 0.70) ‚úì
- Precision : [X.XX] (objectif : > 0.60) ‚úì

**Facteurs pr√©dictifs cl√©s identifi√©s** :
1. Nombre de retards de paiement ant√©rieurs
2. Ratio dette/revenu
3. Revenu mensuel
4. Anciennet√© de l'historique de cr√©dit
5. Variables cr√©√©es (ratios financiers)

**Valeur Business** :
- R√©duction potentielle du taux de d√©faut de [X]%
- √âconomies estim√©es √† [X]‚Ç¨ par an
- Temps de traitement des demandes : de 3 jours ‚Üí instantan√©
- Am√©lioration de l'exp√©rience client

### 9.2 Limites du Mod√®le

#### 9.2.1 Limites Techniques

**D√©s√©quilibre des classes** : Malgr√© les techniques de r√©√©quilibrage (SMOTE, ajustement des poids), le mod√®le peut encore sous-performer sur la classe minoritaire.

**Features limit√©es** : L'absence de certaines donn√©es (ex : comportement de paiement mensuel, transactions r√©centes) limite la pr√©cision.

**Donn√©es statiques** : Le mod√®le ne capture pas les √©volutions temporelles (changement de situation professionnelle, √©v√©nements de vie).

**Overfitting potentiel** : Malgr√© la validation crois√©e, le mod√®le pourrait ne pas g√©n√©raliser parfaitement sur des donn√©es futures tr√®s diff√©rentes.

#### 9.2.2 Limites √âthiques et R√©glementaires

**Biais potentiels** : 
- Biais de s√©lection : donn√©es uniquement sur cr√©dits pass√©s
- Biais d√©mographiques : le mod√®le peut reproduire des discriminations historiques
- Analyse de fairness n√©cessaire (parit√© d√©mographique, √©galit√© des chances)

**Explicabilit√©** :
- Les mod√®les complexes (XGBoost) sont moins interpr√©tables
- N√©cessit√© d'outils compl√©mentaires (SHAP) pour la transparence
- Conformit√© RGPD : droit √† l'explication

**Zone grise juridique** :
- Utilisation de variables sensibles (√¢ge, genre) peut √™tre interdite dans certains pays
- N√©cessit√© d'un audit juridique avant d√©ploiement

#### 9.2.3 Limites Business

**Co√ªt des erreurs asym√©trique** : Un faux n√©gatif co√ªte 20x plus cher qu'un faux positif ‚Üí le seuil de d√©cision doit √™tre ajust√©.

**√âvolution du contexte √©conomique** : Une r√©cession ou crise peut rendre le mod√®le obsol√®te rapidement.

**Acceptabilit√© client** : Certains clients peuvent contester les d√©cisions automatis√©es.

### 9.3 Pistes d'Am√©lioration

#### 9.3.1 Am√©liorations Techniques

**Enrichissement des donn√©es** :
- Int√©grer des donn√©es externes : bureaux de cr√©dit, r√©seaux sociaux (avec consentement)
- Donn√©es temporelles : historique de transactions sur 12 mois
- Donn√©es alternatives : paiement loyer, factures utilities

**Techniques avanc√©es** :
- Stacking/Ensembling : combiner les pr√©dictions de plusieurs mod√®les
- Deep Learning : r√©seaux de neurones pour capturer des interactions complexes
- AutoML : automatiser la s√©lection et l'optimisation des mod√®les

**Calibration des probabilit√©s** :
- Utiliser Platt Scaling ou Isotonic Regression pour am√©liorer la calibration
- Les probabilit√©s pr√©dites refl√©teraient mieux les vraies probabilit√©s

**Gestion du d√©s√©quilibre** :
- Tester d'autres techniques : ADASYN, BorderlineSMOTE
- Apprentissage √† co√ªt sensible (cost-sensitive learning)
- Ajuster le seuil de d√©cision selon une analyse co√ªt-b√©n√©fice

#### 9.3.2 Am√©liorations du Feature Engineering

**Variables temporelles** :
- Tendances sur 3, 6, 12 mois (revenu, d√©penses)
- Saisonnalit√© des comportements financiers
- Ratio de croissance du revenu

**Agr√©gations avanc√©es** :
- Clustering de comportements (segments de clients)
- Scores composites pond√©r√©s par importance SHAP
- Interactions de 3√®me niveau

**Donn√©es textuelles** :
- NLP sur les motifs de demande de cr√©dit
- Analyse de sentiment des commentaires clients

#### 9.3.3 Validation et Monitoring

**Validation temporelle** :
- Train sur donn√©es 2020-2022, valider sur 2023, tester sur 2024
- V√©rifier la stabilit√© des performances dans le temps

**A/B Testing** :
- D√©ployer le mod√®le sur 10% des demandes
- Comparer performances vs processus manuel
- It√©rer selon feedback

**Monitoring continu** :
- Dashboard temps r√©el des performances (AUC, Recall, taux de d√©faut r√©el)
- Alertes si drift d√©tect√© (changement de distribution)
- R√©entra√Ænement trimestriel avec nouvelles donn√©es

#### 9.3.4 Aspects √âthiques et R√©glementaires

**Audit de fairness** :
- Mesurer les disparit√©s entre groupes d√©mographiques
- Appliquer des contraintes de parit√© si n√©cessaire
- Documentation transparente des biais identifi√©s

**Explicabilit√© renforc√©e** :
- Interface utilisateur montrant les facteurs de d√©cision
- Argumentaire clair pour chaque refus
- Possibilit√© de contestation humaine

**Gouvernance** :
- Comit√© d'√©thique pour superviser l'utilisation du mod√®le
- Audits r√©guliers par tiers ind√©pendants
- Conformit√© GDPR, B√¢le III, directives BCE

#### 9.3.5 D√©ploiement en Production

**Architecture** :
- API REST avec FastAPI ou Flask
- Conteneurisation avec Docker
- Orchestration avec Kubernetes pour la scalabilit√©

**MLOps** :
- Versioning des mod√®les avec MLflow ou DVC
- Pipeline CI/CD automatis√©
- Tests automatiques (unit tests, integration tests)

**Infrastructure** :
- Cloud (AWS SageMaker, Google Vertex AI, Azure ML)
- Syst√®me de cache pour pr√©dictions fr√©quentes
- Load balancing pour haute disponibilit√©

---

## 10. R√©f√©rences

### Articles Acad√©miques
1. Hand, D. J., & Henley, W. E. (1997). Statistical classification methods in consumer credit scoring: a review. *Journal of the Royal Statistical Society*, 160(3), 523-541.

2. Baesens, B., et al. (2003). Benchmarking state-of-the-art classification algorithms for credit scoring. *Journal of the Operational Research Society*, 54(6), 627-635.

3. Lessmann, S., et al. (2015). Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research. *European Journal of Operational Research*, 247(1), 124-136.

### Documentation Technique
4. Scikit-learn Documentation. https://scikit-learn.org/
5. XGBoost Documentation. https://xgboost.readthedocs.io/
6. SHAP Documentation. https://shap.readthedocs.io/

### Ressources en ligne
7. Kaggle - Credit Scoring Dataset. https://www.kaggle.com/datasets/kapturovalexander/bank-credit-scoring/
8. Towards Data Science - Credit Risk Modeling. https://towardsdatascience.com/

### R√©glementation
9. R√®glement G√©n√©ral sur la Protection des Donn√©es (RGPD). https://gdpr.eu/
10. Basel Committee on Banking Supervision. https://www.bis.org/bcbs/

---

## 11. Annexes

### Annexe A : Structure du Projet GitHub

```
bank-credit-scoring/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                      # Donn√©es brutes
‚îÇ   ‚îú‚îÄ‚îÄ processed/                # Donn√©es pr√©trait√©es
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Description des donn√©es
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA.ipynb             # Analyse exploratoire
‚îÇ   ‚îú‚îÄ‚îÄ 02_Preprocessing.ipynb    # Pr√©traitement
‚îÇ   ‚îú‚îÄ‚îÄ 03_Modeling.ipynb         # Mod√©lisation
‚îÇ   ‚îî‚îÄ‚îÄ 04_Evaluation.ipynb       # √âvaluation
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_processing.py        # Fonctions de preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py    # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ modeling.py               # Classes de mod√®les
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py             # M√©triques et √©valuation
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ logreg_best.pkl          # Mod√®le sauvegard√©
‚îÇ   ‚îú‚îÄ‚îÄ rf_best.pkl
‚îÇ   ‚îî‚îÄ‚îÄ xgb_best.pkl
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ figures/                  # Graphiques g√©n√©r√©s
‚îÇ   ‚îî‚îÄ‚îÄ rapport_final.pdf         # Ce rapport
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                     # Page d'accueil du projet
‚îú‚îÄ‚îÄ .gitignore                    # Fichiers √† ignorer
‚îî‚îÄ‚îÄ LICENSE                       # Licence du projet
```

### Annexe B : Commandes d'Installation

```bash
# Cloner le repository
git clone https://github.com/[username]/bank-credit-scoring.git
cd bank-credit-scoring

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer Jupyter
jupyter notebook
```

### Annexe C : Exemple de requirements.txt

```
pandas==2.1.0
numpy==1.24.0
matplotlib==3.7.1
seaborn==0.12.2
plotly==5.14.1
scikit-learn==1.3.0
xgboost==1.7.6
lightgbm==4.0.0
catboost==1.2
imbalanced-learn==0.11.0
shap==0.42.1
jupyter==1.0.0
notebook==7.0.0
```

### Annexe D : Glossaire

**AUC-ROC** : Area Under the Receiver Operating Characteristic curve. M√©trique mesurant la capacit√© du mod√®le √† distinguer les classes.

**Recall (Sensibilit√©)** : Proportion de vrais positifs correctement identifi√©s. Crucial pour minimiser les faux n√©gatifs.

**Precision** : Proportion de pr√©dictions positives qui sont correctes. Important pour √©viter les faux positifs.

**F1-Score** : Moyenne harmonique de Precision et Recall. √âquilibre entre les deux m√©triques.

**Cross-Validation** : Technique de validation en s√©parant les donn√©es en K parties pour entra√Æner et tester le mod√®le K fois.

**Overfitting** : Le mod√®le apprend trop bien les donn√©es d'entra√Ænement et ne g√©n√©ralise pas sur de nouvelles donn√©es.

**Feature Engineering** : Processus de cr√©ation de nouvelles variables √† partir des variables existantes.

**SMOTE** : Synthetic Minority Over-sampling Technique. M√©thode pour g√©rer le d√©s√©quilibre de classes.

**SHAP** : SHapley Additive exPlanations. Technique d'interpr√©tabilit√© bas√©e sur la th√©orie des jeux.

---

**Date de r√©daction** : D√©cembre 2025  
**Version** : 1.0  
**Contact** : [Votre email]  
**Repository GitHub** : [Lien vers votre repo]

---

*Ce rapport a √©t√© r√©dig√© dans le cadre du module Data Science & Machine Learning (2025-2026) sous la supervision du Professeur A. Larhlimi.*